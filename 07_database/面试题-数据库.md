# 面试题-数据库

#### 有了关系型数据库，为什么还需要NOSQL？

NOSQL 数据库无需提前设计表结构，数据可以根据需要自由地存储和组织，而且相对于关系型数据库，NOSQL 高效灵活，非常适合那些复杂、高变化、高并发量的场景中。

#### 什么是关系型数据库，什么是非关系型数据库？

关系型数据库，是指采用了关系模型来组织数据的数据库（关系模型可以简单理解为二维表格模型），其以行和列的形式存储数据，以便于用户管理。

关系型数据库中有表的概念，表中包含了行和列，多张（或1张）表可以组成数据库。

关系型数据库具有以下几个特点：
1、传统的关系型数据库中的数据以行和列的方式进行存储。 
2、关系型数据库采用结构化查询语言（即SQL）来对数据库进行查询 
3、关系型数据库强调 ACID 规则（原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）、持久性（Durability）） 
4、关系型数据库十分强调数据的一致性，并为此降低读写性能付出了巨大的代价 
5、关系型数据的数据通常存储在硬盘中

我们常见的MySQL、Oracle等都是关系型数据库。但是因为关系型数据库强调强一致性、以及基于硬盘存储，所以存在着一定的性能问题，相比之下，非关系型数据库在这方面就会相对有些优势。

NoSQL 表示非关系型数据库，主要指那些非关系型的、分布式的，且一般不保证 ACID 的数据存储系统，主要代表如 Redis、MongoDB 等。

非关系型数据库的存储方式是基于键值来存储的，对于值的类型也有不同的支持，所以没有固定的要求和限制。

#### 关系型数据库和非关系型数据库主要有哪些区别？

1、关系型数据库以表的形式进行存储数据，而非关系型数据库以 Key-value 的形式存储数据。 
2、关系型数据库需要保证事务的 ACID，而非关系型数据库中的事务一般无法回滚。（也有部分数据库可以回滚，如 MongoDB 在集群模式下） 
3、关系型数据库可以通过一张表中的任意字段进行查询，非关系型数据库需要通过key进行查询 
4、一般来说，关系型数据库是基于硬盘存储，非关系型数据库基于内存存储。 （Mongodb 基于磁盘存储）
5、关系型数据库支持各种范围查询、公式计算等，非关系型数据库不一定支持。

#### MySQL 的数据存储一定是基于硬盘的吗？

不是的，MySQL 也可以基于内存的，即 MySQL 的内存表技术。它允许将数据和索引存储在内存中，从而提高了检索速度和修改数据的效率。优点包括具有快速响应的查询性能和节约硬盘存储空间。此外，使用内存表还可以实现更高的复杂性，从而提高了 MySQL 的整体性能。

> 创建内存表与创建普通表一样，使用CREATE TABLE语句，但需要将存储引擎设置为：ENGINE = MEMORY

#### 什么是数据库存储引擎？

数据库引擎是用于存储、处理和保护数据的核心服务。利用数据库引擎可控制访问权限并快速处理事务，从而满足企业内大多数需要处理大量数据的应用程序的要求。

使用数据库引擎创建用于联机事务处理或联机分析处理数据的关系数据库。这包括创建用于存储数据的表和用于查看、管理和保护数据安全的数据库对象（如索引、视图和存储过程）。

查看 mysql 当前使用什么存储引擎: `show engines`;

查看 mysql 当前默认的存储引擎: `show variables like '%storage_engine%'`;

查看看某个表用了什么引擎: `show create table 表名;`，在显示结果里参数 engine 后面的就表示该表当前用的存储引擎

#### MySQL 的存储引擎是基于表的还是基于数据库的？

表

#### MySQL 中如何指定引擎？

1、创建表时，可以通过ENGINE来指定存储引擎，在 create 语句最后加上“engine=存储引擎;”即可；

```sql
create table table1(id int(11) primary key auto_increment)engine=MyISAM; 
```

2、修改表时，可以使用“alter table 表名 engine=存储引擎;”来指定存储引擎。

```sql
alter table table1 engine=InnoDB; 
```

#### MySQL 支持哪几种执行引擎，有什么区别?

MySQL 是开源的，我们可以基于其源码编写我们自己的存储引擎，有以下几种存储引擎：

MyISAM、InnoDB、NDB、MEMORY，Archieve、Fedarated 以及 Maria 等。对比如下：

![mysql存储引擎对比](./images/mysql存储引擎对比.png)

#### Innodb 和 MyISAM 有什么区别？

InnoDB 和 MyISAM 是 MySQL 中比较常用的两个执行引擎，MySQL 在 5.5 之前版本默认存储引擎是 MyISAM，5.5 之后版本默认存储引擎是 InnoDB，MyISAM 适合查询以及插入为主的应用，InnoDB 适合频繁修改以及涉及到安全性较高的应用。

- 如果应用需要高度的数据完整性和事务支持，那么 **InnoDB** 是更好的选择。**所以频繁修改及数据安全性的情况适合**。
- 如果应用主要是读取操作，或者需要高效的全文搜索功能，那么 MyISAM 可能更适合。**所以查询频繁的适合**。

他们主要有以下区别：

- 一、InnoDB 支持事务，MyISAM 不支持
- 二、InnoDB 是聚集索引，MyISAM 是非聚集索引。MyISAM 是采用了一种索引和数据分离的存储方式，Innodb 的聚簇索引中索引和数据在一起。
- 三、InnoDB 支持外键，MyISAM 不支持
- 四、InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁。
- 五、InnoDB 不支持 FULLTEXT 类型的索引（5.6之前不支持全文索引）
- 六、InnoDB 中不保存表的行数，但是 MyISAM 只要简单的读出保存好的行数即可
- 七、对于自增长的字段，InnoDB 中必须包含只有该字段的索引，但是在 MyISAM 表中可以和其他字段一起建立联合索引
- 八、清空整个表时，InnoDB 是一行一行的删除，效率非常慢。MyISAM 则会重建表

|          | InnoDB             | MyISAM   |
| -------- | ------------------ | -------- |
| 事务     | 支持               | 不支持   |
| 外键     | 支持               | 不支持   |
| 聚簇索引 | 支持               | 不支持   |
| 锁级别   | 支持行级锁、表级锁 | 表级锁   |
| 行数保存 | 不支持             | 支持     |
| 清空方式 | 逐行删除           | 重建表   |
| 默认版本 | 5.5 之后           | 5.5 之前 |
| 全文索引 | 5.6 以后支持       | 支持     |

#### char 和 varchar 的区别？

char 和 varchar 都是用于在数据库中存储字符串的数据类型。它们之间的主要区别在于存储空间的使用方式：

`char` 是一种定长的数据类型，它的长度固定且在存储时会自动在结尾添加空格来将字符串填满指定的长度。`char` 的长度范围是 `0-255`，`varchar` 是一种可变长度的数据类型，它只会存储实际的字符串内容，不会填充空格。因此，在存储短字符串时，varchar 可以节省空间。varchar 的长度范围是 `0-65535`（MySQL 5.0.3 之后的版本）。

> 如果使用 char(100)，则插入记录后就分配了 100 个字符，后续修改不会造成页分裂的问题，而 varchar(100) 由于没有提前分配存储空间，因为表中数据刚开始插入时，可变长度字段值都是根据实际长度存储下来的，且行与行之间数据也是紧密连续存放在文件地址中的。那么现在值变长了，原来的位置无法扩展出新的空间出来，所以无法覆盖存放到原来的位置上。此时 MySQL 就会使用页分裂的方法扩展字段变长的空间。

**varchar 的优点**是变长的字符串类型，兼容性更好；但是同时**也会带来一些问题**，如使用 varchar 可能会产生内存碎片、varchar 会额外需要 1 到 2 个字节存储长度信息、以及 update 语句可能会导致页分裂等。

**char 的优点**是定长的字符串类型，减少内存碎片，并且无需额外的磁盘空间去存储长度信息。但是他的**缺点是**会丢失列末尾的空格信息。

> char 会在长度不足时，在后面补充空格，所以当我要在一个长度为 10 的 char 中，分别存一个 "ping " 和 "ping"时，他存储的都是"ping    "，所以会丢失原来存储的空格信息。

#### MySQL 5.x 和 8.0 有什么区别？

1. 性能：MySQL 8.0 的速度要比 MySQL 5.7 快 2 倍。MySQL 8.0 在以下方面带来了更好的性能：读/写工作负载、IO 密集型工作负载、以及高竞争（"hot spot"热点竞争问题）工作负载。
2. NoSQL：MySQL 从 5.7  版本开始提供 NoSQL 存储功能，目前在 8.0 版本中这部分功能也得到了更大的改进。该项功能消除了对独立的 NoSQL 文档数据库的需求，而 MySQL 文档存储也为 schema-less 模式的 JSON 文档提供了多文档事务支持和完整的 ACID 合规性。
3. 窗口函数(Window Functions)：从 MySQL 8.0 开始，新增了一个叫窗口函数的概念，它可以用来实现若干新的查询方式。窗口函数与 SUM()、COUNT() 这种集合函数类似，但它不会将多行查询结果合并为一行，而是将结果放回多行当中。即窗口函数不需要 GROUP BY。
4. 隐藏索引：在 MySQL 8.0 中，索引可以被“隐藏”和“显示”。当对索引进行隐藏时，它不会被查询优化器所使用。我们可以使用这个特性用于性能调试，例如我们先隐藏一个索引，然后观察其对数据库的影响。如果数据库性能有所下降，说明这个索引是有用的，然后将其“恢复显示”即可；如果数据库性能看不出变化，说明这个索引是多余的，可以考虑删掉。
5. 取消查询缓存：MySQL 8.0 出于性能和可维护性方面的考虑取消了查询缓存，通过使用索引、更好的查询计划优化、以及缓存结果集而不是整个查询。这些方法更有效地提高了查询性能，同时避免了查询缓存可能引入的问题。
6. select for update 支持 no wait：如果另一个事务已经锁定了一些行，当前事务将等待直到那些锁被释放。加上 NOWAIT 关键字后，如果尝试锁定的行已经被其他事务锁定，数据库将立即抛出一个错误，而不是等待。
7. 降序索引：MySQL 8.0 为索引提供按降序方式进行排序的支持，在这种索引中的值也会按降序的方式进行排序。
8. 通用表表达式(Common Table Expressions CTE)：在复杂的查询中使用嵌入式表时，使用 CTE 使得查询语句更清晰。
9. UTF-8 编码：从 MySQL 8 开始，使用 utf8mb4 作为 MySQL 的默认字符集。
10. JSON：MySQL 8 大幅改进了对 JSON 的支持，添加了基于路径查询参数从 JSON 字段中抽取数据的 JSON_EXTRACT() 函数，以及用于将数据分别组合到 JSON 数组和对象中的 JSON_ARRAYAGG() 和 JSON_OBJECTAGG() 聚合函数。
11. 可靠性：InnoDB 现在支持表 DDL 的原子性，也就是 InnoDB 表上的 DDL 也可以实现事务完整性，要么失败回滚，要么成功提交，不至于出现 DDL 时部分成功的问题，此外还支持 crash-safe 特性，元数据存储在单个事务数据字典中。
12. 高可用性(High Availability)：InnoDB 集群为您的数据库提供集成的原生 HA 解决方案。
13. 安全性：对 OpenSSL 的改进、新的默认身份验证、SQL 角色、密码强度、授权。

> 详见：https://dev.mysql.com/blog-archive/whats-new-in-mysql-8-0-generally-available/

#### 什么是数据库范式，为什么要反范式？

所谓数据库范式，其实就是数据库的设计上的一些规范；这些规范可以让数据库的设计更加简洁、清晰；同时也会更加好的可以保证一致性。

三个常用的范式：

- 第一范式（1NF）是说，数据库表中的属性的原子性的，要求属性具有原子性，不可再被拆分。
  - 比如地址如果都细化拆分成省、市、区、街道、小区等等多个字段这就是符合第一范式的， 如果地址就是一个字段，那就不符合了。
- 第二范式（2NF）是说，数据库表中的每个实例或记录必须可以被唯一地区分，说白了就是要有主键，其他的字段都依赖于主键。
- 第三范式（3NF）是说，任何非主属性不依赖于其它非主属性，也就是说，非主键外的所有字段必须互不依赖。

> 如果我们在做表结构设计的时候，完全遵守数据库三范式，确实可以避免一些写时异常，提升一些写入性能，但是同时也会丢失一些读取性能。
>
> 因为在遵守范式的数据库设计中，表中不能有任何冗余字段，这就使得查询的时候就会经常有多表关联查询，这无疑是比较耗时的。

于是就有了反范式化。所谓反范式化，是一种针对遵从设计范式的数据库的性能优化策略。

也就是说，反范式化不等于非范式化，反范式化一定发生在满足范式设计的基础之上。**前者相当于先遵守所有规则，再进行局部调整。**

比如我们可以在表中增加一些冗余字段，方便我们进行数据查询，而不再需要经常做多表 join，但同时，这也会带来一个问题，那就是这些冗余字段之间的一致性如何保证，这个问题本来在遵守范式的设计中是不会有的，一旦做了反范式，那就需要开发者自行解决了。

反范式其实本质上是软件开发中一个比较典型的方案，那就是**"用空间换时间"**，通过做一些数据冗余，来提升查询速度。

在互联网业务中，比较典型的就是数据量大，并发高，并且通常查询的频率要远高于写入的频率，所以适当的做一些反范式，通过做一些字段的冗余，可以提升查询性能，降低响应时长，从而提升并发度。

#### 为什么大厂不建议使用多表 join？

MySQL 是使用了嵌套循环（Nested-Loop Join）的方式来实现关联查询的，简单点说就是要通过两层循环，用第一张表做外循环，第二张表做内循环，外循环的每一条记录跟内循环中的记录作比较，符合条件的就输出。

而具体到算法实现上主要有 `simple nested loop`，`block nested loop` 和 `index nested loop` 这三种。而且这三种的效率都没有特别高。

MySQL 是使用了嵌套循环（Nested-Loop Join）的方式来实现关联查询的，如果有 2 张表 join 的话，复杂度最高是 `O(n^2)`，3 张表则是 `O(n^3)`...随着表越多，表中的数据量越多， JOIN 的效率会呈指数级下降。

> MySQL 8.0 中新增了 hash join 算法

#### MySQL 的 Hash Join 是什么？

hash join 是 MySQL 8.0.18 版本中新推出的一种多表 join 的算法。

> 在这之前，MySQL 是使用了嵌套循环（Nested-Loop Join）的方式来实现关联查询的，而嵌套循环的算法其实性能是比较差的，而 Hash Join 的出现就是要优化 Nested-Loop Join 的。

所谓 Hash Join，其实是因为他底层用到了 Hash 表。

Hash Join 是一种针对 equal-join 场景的优化，他的基本思想是将驱动表数据加载到内存，并建立 hash 表，这样只要遍历一遍非驱动表，然后再去通过哈希查找在哈希表中寻找匹配的行 ，就可以完成 join 操作了。

但是，内存是有限的（通过 `join_buffer_size` 限制），**如果内存中存不下驱动表的数据怎么办呢？**

#### 基于磁盘的 hash join

如果驱动表中的数据量比较大， 没办法一次性的加载到内存中，就需要考虑把这些数据存储在磁盘上。通过将哈希表的一部分存储在磁盘上，分批次地加载和处理数据，从而减少对内存的需求。

在这样的算法中，为了避免一个大的 hash 表内存中无法完全存储，那么就采用分表的方式来实现，即首先利用 hash 算法将驱动表进行分表，并产生临时分片写到磁盘上。

这样就相当于把一张驱动表，拆分成多个 hash 表，并且分别存储在磁盘上。

接下来就是做 join 了，在这个过程中，会对被驱动表使用同样的 hash 算法进行分区，确定好在哪个分区之后，先确认下这个分区是否已经加载到内存，如果已经加载，则直接在内存中的哈希表中进行查找匹配的行。

如果哈希值对应的分区尚未加载到内存中，则从磁盘上读取该分区的数据到内存中的哈希表，并进行匹配。

就这样不断的重复进行下去，直到把所有数据都 join 完，把结果集返回。

#### 说一说 MySQL 一条 SQL 语句的执行过程？

![一条sql语句的执行过程](./images/一条sql语句的执行过程.png)

1. 使用连接器，通过客户端/服务器通信协议与 MySQL 建立连接。并查询是否有权限
2. Mysql8.0 之前检查是否开启缓存，开启了 Query Cache 且命中完全相同的 SQL 语句，则将查询结果直接返回给客户端；
3. 由解析器（分析器)进行语法分析和语义分析，并生成解析树。如查询是 select、表名users、条件是 `age='18' and name='ping'`，预处理器则会根据 MySQL 规则进一步检查解析树是否合法。比如检查要查询的数据表或数据列是否存在等。
4. 由优化器生成执行计划。根据索引看看是否可以优化
5. 执行器来执行 SQL 语句，这里具体的执行会操作MySQL的存储引擎来执行 SQL 语句，根据存储引擎类型，得到查询结果。若开启了 Query Cache，则缓存，否则直接返回。

#### InnoDB 支持哪几种行格式？

数据库表的行格式决定了一行数据是如何进行物理存储的，进而影响查询和 DML 操作的性能。

在 InnoDB 中，常见的行格式有 4 种：

- **COMPACT** ：是 MySQL 5.0 之前的默认格式，除了保存字段值外，还会利用空值列表保存 null 值，还会记录变长字段长度列表和记录头信息。

![mysql行格式-COMPACT](./images/mysql行格式-COMPACT.png)

> 对于可变长度列，前 768 字节的数据存储在B树节点的索引记录中，超出部分存储在溢出页中。大于或等于 768 字节的固定长度列会被编码为可变长度列，并可以存储在页外。

- **REDUNDANT** ：Redundant 是 MySQL5.0 版本之前 InnoDB 的行记录存储方式，用的比较少，Redundant 行格式会把该条记录中所有列 (包括隐藏列) 的长度信息都存储到 '字段长度偏移列表' 中。

![mysql行格式-REDUNDANT](./images/mysql行格式-REDUNDANT.png)

- **DYNAMIC**：DYNAMIC 格式在 MySQL 5.7 版本引入，是 COMPACT 格式的改进版。它保持了 COMPACT 格式的优点，同时在存储大的可变长度列时更加灵活，能够动态地选择存储在页内或页外。DYNAMIC 格式适用于大部分的应用场景，并在存储空间和性能上做了一定的平衡。其结构和 COMPACT 大致相同；

- **COMPRESSED**：是 MySQL 5.1 中 InnoDB 的新特性之一，它可以在存储数据时对数据进行压缩，从而减小磁盘占用空间。它的缺点是增加了 CPU 的使用，可能会降低一些查询的性能。COMPRESSED 行格式是在 DYNAMIC 行格式的基础上添加了页外压缩功能。在存储时，如果发现数据可以通过压缩减小存储空间，就会使用压缩方式来存储数据。在查询时，会自动解压缩数据并返回结果。
- **BARRACUDA**：是 MySQL 5.7 引入的一种新格式，相对于前面的格式，支持更多的高级特性，如动态行格式、行级压缩、空间管理等。

| 行格式     | 紧凑的存储特性 | 增强的可变长度列存储 | 大索引键前缀支持 | 压缩支持 | 支持的表空间类型                | 所需文件格式          |
| ---------- | -------------- | -------------------- | ---------------- | -------- | ------------------------------- | --------------------- |
| REDUNDANT  | 否             | 否                   | 否               | 否       | system, file-per-table, general | Antelope or Barracuda |
| COMPACT    | 是             | 否                   | 否               | 否       | system, file-per-table, general | Antelope or Barracuda |
| DYNAMIC    | 是             | 是                   | 是               | 否       | system, file-per-table, general | Barracuda             |
| COMPRESSED | 是             | 是                   | 是               | 是       | file-per-table, general         | Barracuda             |

#### 什么是数据库事务？

数据库事务 (transaction) 是访问并可能操作各种数据项的一个数据库操作序列，这些操作要么全部执行，要么全部不执行，是一个不可分割的工作单位。 事务由事务开始与事务结束之间执行的全部数据库操作组成。

并非任意的对数据库的操作序列都是数据库事务。事务应该具有4个属性：原子性、一致性、隔离性、持久性。这四个属性通常称为ACID特性。

原子性（Atomicity）：事务作为一个整体被执行，包含在其中的对数据库的操作要么全部被执行，要么都不执行。

一致性（Consistency）：事务应确保数据库的状态从一个一致状态转变为另一个一致状态。一致状态的含义是数据库中的数据应满足完整性约束。

隔离性（Isolation）：多个事务并发执行时，一个事务的执行不应影响其他事务的执行。

持久性（Durability）：一个事务一旦提交，他对数据库的修改应该永久保存在数据库中。

#### InnoDB的一次更新事务是怎么实现的？

一次 InnnoDB 的 update 操作，涉及到 BufferPool、BinLog、UndoLog、RedoLog 以及物理磁盘，完整的一次操作过程基本如下：

**1、在 Buffer Pool 中读取数据：**当 InnoDB 需要更新一条记录时，首先会在 Buffer Pool 中查找该记录是否在内存中。如果没有在内存中，则从磁盘读取该页到 Buffer Pool 中。

**2、记录 UndoLog：**在修改操作前，InnoDB 会在 Undo Log 中记录修改前的数据。Undo Log 是用来保证事务原子性和一致性的一种机制，用于在发生事务回滚等情况时，将修改操作回滚到修改前的状态，以达到事务的原子性和一致性。UndoLog 的写入最开始写到内存中的，然后由1个后台线程定时刷新到磁盘中的。

**3、在 Buffer Pool 中更新：**当执行 update 语句时，InnoDB 会先更新已经读取到 Buffer Pool 中的数据，而不是直接写入磁盘。同时，InnoDB 会将修改后的数据页状态设置为“脏页”（Dirty Page）状态，表示该页已经被修改但尚未写入磁盘。

**4、记录 RedoLog Buffer：**InnoDB 在 Buffer Pool 中记录修改操作的同时，InnoDB 会先将修改操作写入到 redo log buffer 中。

**5、提交事务：**在执行完所有修改操作后，事务被提交。在提交事务时，InnoDB 会将 Redo Log 写入磁盘，以保证事务持久性。

**6、写入磁盘：**在提交过程后，InnoDB 会将 Buffer Pool 中的脏页写入磁盘，以保证数据的持久性。但是这个写入过程并不是立即执行的，是有一个后台线程异步执行的，所以可能会延迟写入，总之就是 MYSQL 会选择合适的时机把数据写入磁盘做持久化。

**7、记录 Binlog：**在提交过程中，InnoDB 会将事务提交的信息记录到 Binlog 中。Binlog 是 MySQL 用来实现主从复制的一种机制，用于将主库上的事务同步到从库上。在 Binlog 中记录的信息包括：事务开始的时间、数据库名、表名、事务 ID、SQL 语句等。

![mysql的一次更新事务](./images/mysql的一次更新事务.png)

需要注意的是，在 binlog 和 redolog 的写入过程中，其实是分成了 2 阶段的，通过 2 阶段提交的方式来保证一致性的。

#### 什么是脏读、幻读、不可重复读？

脏读：读到了其他事务还没有提交的数据。
不可重复读：对某数据进行读取过程中，有其他事务对数据进行了修改（UPDATE、DELETE)，导致第二次读取的结果不同。 
幻读：事务在做范围查询过程中，有另外一个事务对范围内新增了记录(INSERT)，导致范围查询的结果条数不一致。

#### MySQL 中的事务隔离级别？

SQL-92 标准还定义了 4 种隔离级别来解决脏读、幻读、不可重复读等这些异常情况，从高到底依次为：**可串行化(Serializable)**、**可重复读(Repeatable reads)**、**提交读(Read committed)**、**未提交读(Read uncommitted)**。

- 未提交读(Read uncommitted) 是最低的隔离级别。通过名字我们就可以知道，在这种事务隔离级别下，一个事务可以读到另外一个事务未提交的数据。这种隔离级别下会存在幻读、不可重复读和脏读的问题。

- 提交读(Read committed) 也可以翻译成读已提交，通过名字也可以分析出，在一个事务修改数据过程中，如果事务还没提交，其他事务不能读该数据。所以，这种隔离级别是可以避免脏读的发生的。

- 可重复读(Repeatable reads)，由于提交读隔离级别会产生不可重复读的读现象。所以，比提交读更高一个级别的隔离级别就可以解决不可重复读的问题。这种隔离级别就叫可重复读。但是这种隔离级别没办法彻底解决幻读。

- 可串行化(Serializable) 是最高的隔离级别，前面提到的所有的隔离级别都无法解决的幻读，在可串行化的隔离级别中可以解决。

| 隔离级别                                | 脏读 | 不可重复读 | 幻读 |
| --------------------------------------- | ---- | ---------- | ---- |
| 能读到未提交的数据 RU, READ-UNCOMMITTED | Y    | Y          | Y    |
| 能读到已提交的数据 RC，READ-COMMITTED   | N    | Y          | Y    |
| 可重复读 RR, REPEATABLEREAD             | N    | N          | Y    |
| 串行执行 SERIALIZABLE                   | N    | N          | N    |

> 注意！！！以上是 SQL-92 标准中对事务隔离级别以及需要解决的读现象（脏读、幻读、不可重复读）的定义，但是不同的数据库在实际实现的过程中，是有一些细微差异的。

1、查看当前会话隔离级别

`select @@tx_isolation;`

在 MySQL 8.0 中：`SELECT @@transaction_isolation;`

2、查看系统当前隔离级别

`select @@global.tx_isolation;`

3、设置当前会话隔离级别

`set session transaction isolatin level repeatable read;`

4、设置系统当前隔离级别

5、命令行，开始事务时

`set autocommit=off` 或者 `start transaction`

#### InnoDB 如何解决脏读、不可重复读和幻读的？

在 Innodb 中，通过 **MVCC 解决脏读和不可重复读**，通过 **MVCC + 间隙锁解决幻读**的。

脏读的解决。脏读指一个事务可以读取另一个事务未提交的数据，导致数据不一致。在读已提交（Read Committed）隔离级别下，事务只能读取到其他事务已经提交的数据版本。因此，如果一个事务在读取数据时，另一个事务已经修改了这些数据但尚未提交，那么读取事务将不会看到这些未提交的更改。

当事务在“读已提交”隔离级别下执行读取操作时，InnoDB 获取当前最新的全局事务 ID，这个 ID 表示在当前时刻所有已提交事务的最新状态。InnoDB会检查每个数据行的版本，如果该版本是由一个小于或等于当前事务 ID 的事务修改的，并且该事务已提交，则这个版本是可见的。这保证了事务只能看到在它开始之前已经提交的数据版本。

不可重读的解决。不可重复读指一个事务读取同一行数据两次，但是在两次读取之间另一个事务修改了该行数据，导致两次读取的结果不同。InnoDB 通过使用 MVCC 来解决不可重复读的问题。在 RR 这种隔离级别下，当我们使用快照读进行数据读取的时候，只会在第一次读取的时候生成一个 Read View，后续的所有快照读都是用的同一个快照，所以就不会发生不可重复读的问题了。

幻读的解决。InnoDB 的 RR 级别中，基于 MVCC + 间隙锁，是在某种程度上是可以避免幻读的发生的，但是没有办法完全避免，当一个事务中发生当前读的时候，会导致幻读的发生。

#### Innodb 的 RR 到底有没有解决幻读？

InnoDB 中的 REPEATABLE READ 这种隔离级别通过 **间隙锁 + MVCC** 解决了大部分的幻读问题，但是并不是所有的幻读都能解读，想要彻底解决幻读，需要使用 Serializable 的隔离级别。

RR 中，通过间隙锁解决了部分当前读的幻读问题，通过增加间隙锁将记录之间的间隙锁住，避免新的数据插入。

RR 中，通过 MVCC 机制的，解决了快照读的幻读问题，RR 中的快照读只有第一次会进行数据查询，后面都是直接读取快照，所以不会发生幻读。

但是，如果两个事务，事务1 先进行快照读，然后事务2 插入了一条记录并提交，再在事务1 中进行 update 新插入的这条记录是可以更新出成功的，这就是发生了幻读。

还有一种场景，如果两个事务，事务1 先进行快照读，然后事务2 插入了一条记录并提交，在事务1 中进行了当前读之后，再进行快照读也会发生幻读。

#### MVCC 解决幻读

MVCC，是 Multiversion Concurrency Control 的缩写，翻译过来是多版本并发控制，和数据库锁一样，他也是一种并发控制的解决方案。它主要用来解决读-写并发的情况。

我们知道，在 MVCC 中有两种读，一种是快照读、一种是当前读。

所谓快照读，就是读取的是快照数据，即快照生成的那一刻的数据，像我们常用的普通的 SELECT 语句在不加锁情况下就是快照读。

`SELECT * FROM xx_table WHERE ...`

在 RC 中，每次读取都会重新生成一个快照，总是读取行的最新版本。
在 RR 中，快照会在事务中第一次 SELECT 语句执行时生成，只有在本事务中对数据进行更改才会更新快照。

那么也就是说，如果在 RR 下，一个事务中的多次查询，是不会查询到其他的事务中的变更内容的，所以，也就是可以解决幻读的。

如果我们把事务隔离级别设置为 RR，那么因为有了 MVCC 的机制，就能解决幻读的问题。

#### 间隙锁与幻读

上面我们讲过了 MVCC 能解决 RR 级别下面的快照读的幻读问题，那么当前读下面的幻读问题怎么解决呢？

当前读就是读取最新数据，所以，加锁的 SELECT，或者对数据进行增删改都会进行当前读，比如：

```sql
SELECT * FROM xx_table LOCK IN SHARE MODE;

SELECT * FROM xx_table FOR UPDATE;

INSERT INTO xx_table ...

DELETE FROM xx_table ...

UPDATE xx_table ...
```

在 RR 的级别下，当我们使用 `SELECT … FOR UPDATE `的时候，会进行加锁，不仅仅会对行记录进行加锁，还会对记录之间的间隙进行加锁，这就叫做间隙锁。

#### 解决不了的幻读

前面我们介绍了快照读（无锁查询）和当前读（有锁查询）下是如何解决幻读的问题的，但是，上面的例子就是幻读的所有情况了吗？显然并不是。

我们说 MVCC 只能解决快照读的幻读，那如果在一个事务中发生了当前读，并且在另一个事务插入数据前没来得及加间隙锁的话，会发生什么呢？

![解决不了的幻读-1](./images/解决不了的幻读-1.png)

在上面的例子中，在事务1 中，我们并没有在事务开启后立即加锁，而是进行了一次普通的查询，然后事务2 插入数据成功之后，再通过事务1进行了2次查询。

我们发现，事务1后面的两次查询结果完全不一样，没加锁的情况下，就是快照读，读到的数据就和第一次查询是一样的，就不会发生幻读。但是第二次查询加了锁，就是当前读，那么读取到的数据就有其他事务提交的数据了，就发生了幻读。

那么，如果你理解了上面的这个例子，并且你也理解了当前读的概念，那么你很容易就能想到，下面的这个CASE其实也是会发生幻读的：

![解决不了的幻读-2](./images/解决不了的幻读-2.png)

这里发生幻读的原理，和上面的例子其实是一样的，那就是 MVCC 只能解决快照读中的幻读问题，而对于当前读（SELECT FOR UPDATE、UPDATE、DELETE 等操作）还是会产生幻读的现象的。即，在同一个事务里面，如果既有快照读，又有当前读，那是会产生幻读的、

UPDATE 语句也是一种当前读，所以它是可以读到其他事务的提交结果的。

为什么事务1 的最后一次查询和倒数第二次查询的结果也不一样呢？

是因为根据快照读的定义，在 RR 中，如果本事务中发生了数据的修改，那么就会更新快照，那么最后一次查询的结果也就发生了变化。

#### 如何避免幻读?

那么了解了幻读的解决场景，以及不能解决的几个 CASE 之后，我们来总结一下该如何解决幻读的问题呢？

首先，如果想要彻底解决幻读的问题，在 InnoDB 中只能使用 Serializable 这种隔离级别。

那么，如果想在一定程度上解决或者避免发生幻读的话，使用 RR 也可以，但是 RC、RU 肯定是不行的。

在 RR 级别中，能使用快照读（无锁查询）的就使用快照读，这样不仅可以减少锁冲突，提升并发度，而且还能避免幻读的发生。

那么，如果在并发场景中，一定要加锁的话怎么办呢？那就一定要在事务一开始就立即加锁，这样就会有间隙锁，也能有效的避免幻读的发生。
但是需要注意的是，间隙锁是导致死锁的一个重要根源~所以，用起来也需要慎重。

#### 为什么 MySQL 默认使用 RR 隔离级别？

对于数据库的默认隔离级别，Oracle 默认的隔离级别是 RC，而 MySQL 默认的隔离级别是 RR。

那么，你知道为什么 Oracle 选择 RC 作为默认级别，而 MySQL 要选择 RR 作为默认的隔离级别吗？

**Oracle 的隔离级别**

Oracle 只支持 ANSI/ISO SQL 定义的 Serializable 和 Read Committed，其实，根据 Oracle 官方文档给出的介绍，Oracle 支持三种隔离级别：


即 Oracle 支持 Read Committed、Serializable 和 Read-Only。

Read-Only 只读隔离级别类似于 Serializable 隔离级别，但是只读事务不允许在事务中修改数据，除非用户是 SYS。

在 Oracle 这三种隔离级别中，Serializable 和 Read-Only 显然都是不适合作为默认隔离级别的，那么就只剩 Read Committed 这个唯一的选择了。

**MySQL 的隔离级别**

相比于 Oracle，MySQL 的默认隔离级别的可选范围就比较大了。

首先，我们先从四种隔离级别中排除 Serializable 和 Read Uncommitted 这两种，主要是因为这两个级别一个隔离级别太高，一个太低。太高的就会影响并发度，太低的就有脏读现象。

那么，剩下的 RR 和 RC 两种，怎么选？

在 MySQL 设计之初，他的定位就是提供一个稳定的关系型数据库。而为了要解决 MySQL 单点故障带来的问题，MySQL 采用主从复制的机制。

所谓主从复制，其实就是通过搭建 MySQL 集群，整体对外提供服务，集群中的机器分为主服务器（Master）和从服务器（Slave），主服务器提供写服务，从服务器提供读服务。

为了保证主从服务器之间的数据的一致性，就需要进行数据同步，大致的同步过程如下，这里就不详细介绍了

![mysql的主从复制](./images/mysql的主从复制.png)


MySQL 在主从复制的过程中，数据的同步是通过 bin log 进行的，简单理解就是主服务器把数据变更记录到 bin log 中，然后再把 bin log 同步传输给从服务器，从服务器接收到 bin log之后，再把其中的数据恢复到自己的数据库存储中。

那么，binlog 里面记录的是什么内容呢？格式是怎样的呢？

MySQL 的 bin log 主要支持三种格式，分别是 statement、row 以及 mixed。MySQL 是在 5.1.5 版本开始支持 row 的、在 5.1.8 版本中开始支持 mixed。

statement 和 row 最大的区别，当 binlog 的格式为 statement 时，binlog 里面记录的就是 SQL 语句的原文（这句话很重要！！！后面会用的到）。

因为 MySQL 早期只有 statement 这种 bin log 格式，这时候，如果使用提交读 (Read Committed)、未提交读 (Read Uncommitted) 这两种隔离级别会出现问题。

> 举个例子，有一个数据库表t1，表中有如下两条记录：

```sql
CREATE TABLE `t1` (
  `a` int(11) DEFAULT NULL,
  `b` int(11) DEFAULT NULL,
  KEY `b` (`b`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1;

insert into t1 values(10,1);
```

接着开始执行两个事务的写操作：

| Session1                                                     | Session2                                                |
| ------------------------------------------------------------ | ------------------------------------------------------- |
| set session transaction isolation level read committed;<br/>设置会话事务隔离级别 读取已提交; |                                                         |
| set autocommit = 0;                                          | set session transaction isolation level read committed; |
|                                                              | set autocommit = 0;                                     |
| begin;                                                       | begin;                                                  |
| delete from t1 where b < 100;                                |                                                         |
|                                                              | insert into t1 values(10,99);                           |
|                                                              | commit;                                                 |
| commit;                                                      |                                                         |

以上两个事务执行之后，数据库里面的记录会只有一条记录（10,99），这个发生在主库的数据变更大家都能理解。

即使 Session 1 的删除操作在 Session 2 的插入操作之后提交，由于 READ COMMITTED 的隔离级别，Session 2 的插入操作不会看到 Session 1 的删除操作，所以最后数据库中仍然会留下 Session 2 插入的记录 (10,99)。

> 以上两个事务执行之后，会在 bin log 中记录两条记录，因为事务2 先提交，所以 `insert into t1 values(10,99);` 会被优先记录，然后再记录 `delete from t1 where b < 100;` （再次提醒：statement 格式的 bin log 记录的是 SQL 语句的原文）
>
> 这样 bin log 同步到备库之后，SQL 语句回放时，会先执行 `insert into t1 values(10,99);`，再执行 `delete from t1 where b < 100;`。
>
> 这时候，数据库中的数据就会变成 EMPTY SET，即没有任何数据。这就导致主库和备库的数据不一致了！！！

为了避免这样的问题发生。MySQL 就把数据库的默认隔离级别设置成了 Repetable Read，那么，Repetable Read 的隔离级别下是如何解决这样问题的那？

那是因为 Repetable Read 这种隔离级别，会在更新数据的时候不仅对更新的行加行级锁，还会增加 GAP 锁和临键锁。上面的例子，在事务2 执行的时候，因为事务1 增加了 GAP 锁和临键锁，就会导致事务2 执行被卡住，需要等事务1提交或者回滚后才能继续执行。

**除了设置默认的隔离级别外，MySQL 还禁止在使用 statement 格式的 bin log 的情况下，使用 READ COMMITTED 作为事务隔离级别。**

一旦用户主动修改隔离级别，尝试更新时，会报错：

```
ERROR 1598 (HY000): Binary logging not possible. Message: Transaction level 'READ-COMMITTED' in InnoDB is not safe for binlog mode 'STATEMENT'
```

所以，现在我们知道了，为什么 MySQL 选择 RR 作为默认的数据库隔离级别了吧，其实就是为了兼容历史上的那种 statement 格式的 bin log。

#### 为什么很多大厂会把默认的 RR 改成 RC

虽然RR的隔离级别可以在一定程度上避免脏读、不可重复读和幻读等问题，但是，对于很多大型的互联网来说，会愿意将数据库的默认隔离级别调整成并发度更高的 RC 级别，从而，提升并发度并且降低发生死锁的概率。

**互联网公司和传统企业最大的区别是什么？**

**高并发！**

没错，互联网业务的并发度比传统企业要高处很多。2020 年双十一当天，订单创建峰值达到 58.3 万笔/秒。

很多人问，要怎么做才能扛得住这么大的并发量。其实，这背后的优化多到几个小时都讲不完，因为要做的、可以做的事情实在是太多了。

而有一个和我们今天这篇文章有关的优化，那就是通过修改数据库的隔离级别来提升并发度。

为什么 RC 比 RR 的并发度要好呢？

首先，RC 在加锁的过程中，是不需要添加 Gap Lock 和 Next-Key Lock 的，只对要修改的记录添加行级锁就行了。

这就使得并发度要比 RR 高很多。

另外，因为 RC 还支持"半一致读"，可以大大的减少了更新语句时行锁的冲突；对于不满足更新条件的记录，可以提前释放锁，提升并发度。

**减少死锁**

因为RR这种事务隔离级别会增加 Gap Lock 和 Next-Key Lock，这就使得锁的粒度变大，那么就会使得死锁的概率增大。

死锁：一个事务锁住了表A，然后又访问表B；另一个事务锁住了表B，然后企图访问表A；这时就会互相等待对方释放锁，就导致了死锁。

RR 和 RC 主要在加锁机制、主从同步以及一致性读方面存在一些差异。

而很多大厂，为了提升并发度和降低死锁发生的概率，会把数据库的隔离级别从默认的 RR 调整成 RC。

当然，这样做也不是完全没有问题，首先使用 RC 之后，就需要自己解决不可重复读的问题，这个其实还好，很多时候不可重复读问题其实是可以忽略的，或者可以用其他手段解决。

比如读取到别的事务修改的值其实问题不太大的，只要修改的时候的不基于错误数据就可以了，所以我们都是在核心表中增加乐观锁标记，更新的时候都要带上锁标记进行乐观锁更新。

还有就是使用 RC 的时候，不能使用 statement 格式的 binlog，这种影响其实可以忽略不计了，因为 MySQL 是在 5.1.5 版本开始支持 row 的、在 5.1.8 版本中开始支持 mixed，后面这两种可以代替 statement 格式。

#### RR 和 RC 的区别

**一致性读**

一致性读，又称为快照读。快照即当前行数据之前的历史版本。快照读就是使用快照信息显示基于某个时间点的查询结果，而不考虑与此同时运行的其他事务所执行的更改。

在 MySQL 中，只有 READ COMMITTED 和 REPEATABLE READ 这两种事务隔离级别才会使用一致性读。

在 RR 中，快照会在事务中第一次 SELECT 语句执行时生成，只有在本事务中对数据进行更改才会更新快照。

在 RC 中，每次读取都会重新生成一个快照，总是读取行的最新版本。

在数据库的 RC 这种隔离级别中，还支持"半一致读" ，一条 update 语句，如果 where 条件匹配到的记录已经加锁，那么 InnoDB 会返回记录最近提交的版本，由 MySQL 上层判断此是否需要真的加锁。

**锁机制**

数据库的锁，在不同的事务隔离级别下，是采用了不同的机制的。在 MySQL 中，有三种类型的锁，分别是 Record Lock、Gap Lock 和 Next-Key Lock。

Record Lock 表示记录锁，锁的是索引记录。

Gap Lock 是间隙锁，锁的是索引记录之间的间隙。

Next-Key Lock 是 Record Lock 和 Gap Lock 的组合，同时锁索引记录和间隙。他的范围是左开右闭的。

在 RC 中，只会对索引增加 Record Lock，不会添加 Gap Lock 和 Next-Key Lock。

在 RR 中，为了解决幻读的问题，在支持 Record Lock 的同时，还支持 Gap Lock 和 Next-Key Lock；

**主从同步**

在数据主从同步时，不同格式的 binlog 也对事务隔离级别有要求。

MySQL 的 binlog 主要支持三种格式，分别是 statement、row 以及 mixed，但是，RC 隔离级别只支持 row 格式的 binlog。如果指定了 mixed 作为 binlog 格式，那么如果使用 RC，服务器会自动使用基于 row 格式的日志记录。

而 RR 的隔离级别同时支持 statement、row 以及 mixed 三种。

#### 当前读和快照读有什么区别？

所谓快照读，就是读取的是快照数据，即快照生成的那一刻的数据，像我们常用的普通的 SELECT 语句在不加锁情况下就是快照读。如：

```sql
SELECT * FROM xx_table WHERE ...
```

和快照读相对应的另外一个概念叫做当前读，当前读就是读取最新数据，所以，加锁的 SELECT，或者对数据进行增删改都会进行当前读，比如：

```sql
SELECT * FROM xx_table LOCK IN SHARE MODE;

SELECT * FROM xx_table FOR UPDATE;

INSERT INTO xx_table ...

DELETE FROM xx_table ...

UPDATE xx_table ...
```

在 MySQL 中，只有 READ COMMITTED 和 REPEATABLE READ 这两种事务隔离级别才会使用快照读。

- 在 RR 中，快照会在事务中第一次 SELECT 语句执行时生成，只有在本事务中对数据进行更改才会更新快照。
- 在 RC 中，每次读取都会重新生成一个快照，总是读取行的最新版本。

#### 如何理解MVCC？

MVCC，是 Multiversion Concurrency Control 的缩写，翻译过来是多版本并发控制，和数据库锁一样，他也是一种并发控制的解决方案。

我们知道，在数据库中，对数据的操作主要有 2 种，分别是读和写，而在并发场景下，就可能出现以下三种情况：

- 读-读并发
- 读-写并发
- 写-写并发

我们都知道，在没有写的情况下读-读并发是不会出现问题的，而写-写并发这种情况比较常用的就是通过加锁的方式实现。那么，读-写并发则可以通过 MVCC 的机制解决。

> 快照读是 MVCC 实现的基础，而当前读是悲观锁实现的基础。
>
> 那么，快照读读到的快照是从哪里读到的呢？换句话说，快照是存在哪里的呢？

**UndoLog**

undo log 是 Mysql 中比较重要的事务日志之一，顾名思义，undo log 是一种用于回退的日志，在事务没提交之前，MySQL 会先记录更新前的数据到 undo log 日志文件里面，当事务回滚时或者数据库崩溃时，可以利用 undo log 来进行回退。

这里面提到的存在 undo log 中的"更新前的数据"就是我们前面提到的快照。所以，这也是为什么很多人说 UndoLog 是 MVCC 实现的重要手段的原因。

那么，一条记录在同一时刻可能有多个事务在执行，那么，undo log 会有一条记录的多个快照，那么在这一时刻发生 SELECT 要进行快照读的时候，要读哪个快照呢？

这就需要用到另外几个信息了。

**行记录的隐式字段**

其实，数据库中的每行记录中，除了保存了我们自己定义的一些字段以外，还有一些重要的隐式字段的：

- `db_row_id`：隐藏主键，如果我们没有给这个表创建主键，那么会以这个字段来创建聚簇索引。 
- `db_trx_id`：对这条记录做了最新一次修改的事务的ID 
- `db_roll_ptr`：回滚指针，指向这条记录的上一个版本，其实他指向的就是Undo Log中的上一个版本的快照的地址。 

因为每一次记录变更之前都会先存储一份快照到 `undo log` 中，那么这几个隐式字段也会跟着记录一起保存在 `undo log` 中，就这样，每一个快照中都有一个 `db_trx_id` 字段表示了对这个记录做了最新一次修改的事务的ID，以及一个 `db_roll_ptr` 字段指向了上一个快照的地址。（`db_trx_id` 和 `db_roll_ptr` 是重点，后面还会用到）

这样，就形成了一个快照链表：

![mvvc和undolog](./images/mvvc和undolog.png)

有了 `undo log`，又有了几个隐式字段，我们好像还是不知道具体应该读取哪个快照，那怎么办呢？

**Read View**

这时候就需要Read View 登场了，

Read View 主要来帮我们解决可见性的问题的, 即他会来告诉我们本次事务应该看到哪个快照，不应该看到哪个快照。

在 Read View 中有几个重要的属性：

- **trx_ids**，系统当前未提交的事务 ID 的列表。
- **low_limit_id**，应该分配给下一个事务的id 值。
- **up_limit_id**，未提交的事务中最小的事务 ID。
- **creator_trx_id**，创建这个 Read View 的事务 ID。

每开启一个事务，我们都会从数据库中获得一个事务 ID，这个事务 ID 是自增长的，通过 ID 大小，我们就可以判断事务的时间顺序。

那么，一个事务应该看到哪些快照，不应该看到哪些快照该如何判断呢？

其实原则比较简单，那就是事务ID大的事务应该能看到事务ID小的事务的变更结果，反之则不能！举个例子：

假如当前有一个事务3想要对某条记录进行一次快照读的时候，他会先创建一个 Read View，并且把当前所有还未提交的事务的信息记录下来。比如 `up_limit_id = 2`，`low_limit_id = 5`，`trx_ids= [2,4,5]`，`creator_trx_id= 3`

我们前面说过，每一条记录上都有一个隐式字段 db_trx_id 记录对这条记录做了最新一次修改的事务的ID，如 db_trx_id = 3；

那么接下来，数据库会拿这条记录 db_trx_id 和 Read View 进行可见性比较。

如果 `db_trx_id < up_limit_id`，则说明，在Read View中所有未提交的事务创建之前，db_trx_id = 3 的这个事务就已经提交了，并且在这期间，并没有新的事务提交。所以，这条记录对当前事务就应该是可见的。

如果，`db_trx_id > low_limit_id`，则说明，db_trx_id = 3 的这个事务是在 Read View 中所有未提交的事务创建之后才提交的，也就是说，在当前事务开启之后，有别的事务修改了数据并作了提交。所以，这个记录对于当前事务来说应该就是不可见的。（不可见怎么办呢？后面讲）

那么，还有另外一种情况，那就是 `up_limit_id < db_trx_id < low_limit_id`，这种情况下，会再拿 `db_trx_id` 和 `Read View` 中的 `trx_ids` 进行逐一比较。

如果，`db_trx_id` 在 `trx_ids` 列表中，那么表示在当前事务开启时，并未提交的某个事务在修改数据之后提交了，那么这个记录对于当前事务来说应该是不可见的。

如果，`db_trx_id` 不在 `trx_ids` 列表中，那么表示的是在当前事务开启之前，其他事务对数据进行修改并提交了，所以，这条记录对当前事务就应该是可见的。

所以，当读取一条记录的时候，经过以上判断，发现记录对当前事务可见，那么就直接返回就行了。那么如果不可见怎么办？没错，那就需要用到 `undo log` 了。

当数据的事务ID 不符合 Read View 规则时候，那就需要从 `undo log` 里面获取数据的历史快照，然后数据快照的事务ID 再来和 `Read View` 进行可见性比较，如果找到一条快照，则返回，找不到则返回空。

> 所以，总结一下，在 InnoDB 中，MVCC 就是通过 Read View + Undo Log 来实现的，undo log 中保存了历史快照，而 Read View 用来判断具体哪一个快照是可见的。

#### MVCC和可重复读

其实，根据不同的事务隔离级别，Read View 的获取时机是不同的，在 RC 下，一个事务中的每一次 SELECT 都会重新获取一次 Read View，而在RR下，一个事务中只在第一次 SELECT 的时候会获取一次 Read View。

所以，可重复读这种事务隔离级别之下，因为有 MVCC 机制，就可以解决不可重复读的问题，因为他只有在第一次 SELECT 的时候才会获取一次 Read View，天然不存在重复读的问题了。

#### 介绍下 InnoDB 的锁机制？

Innodb 中的锁在锁的级别上一版分为两种，一种是共享锁（S锁），一种是排他锁（X锁）。

**共享锁 & 排他锁**

共享锁又称读锁，是读取操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。

排他锁又称写锁，如果事务T对数据A加上排他锁后，则其他事务不能再对A加任任何类型的锁。获得排他锁的事务既能读数据，又能修改数据。

> 这就是我们经常会看到的 X 锁和 S 锁。即排他锁和共享锁。
>
> 除了 S 锁和 X 锁之外，Innodb 还有两种锁，是 IX 锁和 IS 锁，这里的 I 是 Intention 的意思，即意向锁。IX 就是意向排他锁，IS 就是意向共享锁。

**意向锁**

当一个事务请求获取一个行级锁或表级锁时，MySQL 会自动获取相应的表的意向锁。

- IS 锁： 表示事务打算在资源上设置共享锁（读锁）。这通常用于表示事务计划读取资源，并不希望在读取时有其他事务设置排它锁。
- IX 锁： 表示事务打算在资源上设置排它锁（写锁）。这表示事务计划修改资源，并不希望有其他事务同时设置共享或排它锁。

意向锁其实是一个表级锁！

以下是 MySQL 官网上给出的这几种锁之间的冲突关系：

![mysql锁分类](./images/mysql锁分类.png)

**记录锁**

Record Lock，翻译成记录锁，是加在索引记录上的锁。例如，`SELECT c1 FROM t WHERE c1 = 10 For UPDATE;` 会对 c1=10 这条记录加锁，为了防止任何其他事务插入、更新或删除 c1 值为 10 的行。

Record Lock 是一个典型的行级锁，但是需要特别注意的是，Record 锁的不是这行记录，而是锁索引记录。并且 Record lock 锁且只锁索引！

如果没有索引怎么办？对于这种情况，InnoDB 会创建一个隐藏的聚簇索引，并使用这个索引进行记录锁定。

如果我们在一张表中没有定义主键，那么，MySQL 会默认选择一个唯一的非空索引作为聚簇索引。如果没有适合的非空唯一索引，则会创建一个隐藏的主键（row_id）作为聚簇索引。

**插入意向锁**

插入意向锁是一种由插入操作在行插入之前设置的间隙锁。这种锁表明了插入的意图，以这样一种方式，如果多个事务插入到同一索引间隙中但不在间隙内的相同位置插入，则它们不需要相互等待。

假设有索引记录的值为 4 和 7。分别尝试插入值为 5 和 6 的不同事务，在获取插入行的独占锁之前，各自用插入意向锁锁定 4 和 7 之间的间隙，但由于行不冲突，所以它们不会相互阻塞。但是如果他们的都要插入6，那么就会需要阻塞了。

**AUTO-INC 锁**

AUTO-INC 锁是一种特殊的表级锁，由插入带有 AUTO_INCREMENT 列的表的事务获取。在最简单的情况下，如果一个事务正在向表中插入值，任何其他事务都必须等待，以便执行它们自己的插入操作，这样第一个事务插入的行就会接收到连续的主键值。

innodb_autoinc_lock_mode 变量控制用于自增锁定的算法。它允许你选择如何在可预测的自增值序列和插入操作的最大并发性之间进行权衡。

在 MySQL 5.1 之前，AUTO-INC 锁是一种表级锁。

#### MySQL 的行级锁锁的到底是什么？

数据库的行级锁根据锁的粒度不同，可以叫做不同的名字。

Record Lock 表示记录锁，锁的是索引记录。 

Gap Lock 是间隙锁，锁的是索引记录之间的间隙。 

Next-Key Lock 是 Record Lock 和 Gap Lock 的组合，同时锁索引记录和间隙。他的范围是左开右闭的。

> 扩展知识：

**Record Lock**

Record Lock，翻译成记录锁，是加在索引记录上的锁。例如，`SELECT c1 FROM t WHERE c1 = 10 For UPDATE;` 会对 c1=10 这条记录加锁，为了防止任何其他事务插入、更新或删除c1值为 10 的行。

**Gap Lock**

Gap Lock，翻译成间隙锁，他指的是在索引记录之间的间隙上的锁，或者在第一个索引记录之前或最后一个索引记录之后的间隙上的锁。

那么，这里所谓的 Gap（间隙）又怎么理解呢？

Gap 指的是 InnoDB 的索引数据结构中可以插入新值的位置。

当你用语句 `SELECT…FOR UPDATE` 锁定一组行时。InnoDB可以创建锁，应用于索引中的实际值以及他们之间的间隙。例如，如果选择所有大于10的值进行更新，间隙锁将阻止另一个事务插入大于 10 的新值。

既然是锁，那么就可能会影响到数据库的并发性，所以，间隙锁只有在Repeatable Reads这种隔离级别中才会起作用。

在 Repeatable Reads 这种隔离下，对于锁定的读操作（select ... for update 、 lock in share mode)、update操作、delete操作时，会进行如下的加锁：

- 对于具有唯一搜索条件的唯一索引，InnoDB 只锁定找到的索引记录，而不会锁定间隙。 
-  对于其他搜索条件，InnoDB 锁定扫描的索引范围，使用gap lock或next-key lock来阻塞其他事务插入范围覆盖的间隙。 

也就是说，对于 SELECT FOR UPDATE、LOCK IN SHARE MODE、UPDATE 和 DELETE 等语句处理时，除了对唯一索引的唯一搜索外都会获取 gap锁 或 next-key锁，即锁住其扫描的范围。

**Next-Key Lock**

Next-Key 锁是索引记录上的记录锁和索引记录之前间隙上的间隙锁的组合。

![Next-Key-Lock](./images/Next-Key-Lock.png)

设一个索引包含值 10、11、13 和 20。此索引可能的 next-key 锁包括以下区间:

```
(-∞, 10]
(10, 11]
(11, 13]
(13, 20]
(20, ∞ ]
```

对于最后一个间隙，∞ 不是一个真正的索引记录，因此，实际上，这个 next-key 锁只锁定最大索引值之后的间隙。

所以，Next-Key 的锁的范围都是左开右闭的。

Next-Key Lock 和 Gap Lock 一样，只有在 InnoDB 的 RR 隔离级别中才会生效。

#### MySQL 的加锁原则

前面介绍过了 Record Lock、Gap Lock 和 Next-Key Lock，但是并没有说明加锁规则。关于加锁规则，包含了两个“原则”、两个“优化”和一个“bug”：

**原则 1：**加锁的基本单位是 next-key lock。是一个前开后闭区间。 

**原则 2：**查找过程中访问到的对象才会加锁。 

**优化 1：**索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。 

**优化 2：**索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。 

**一个 bug：**唯一索引上的范围查询会访问到不满足条件的第一个值为止。

#### 什么是排他锁和共享锁？

共享锁又称读锁，是读取操作创建的锁。其他用户可以并发读取数据，但任何事务都不能对数据进行修改（获取数据上的排他锁），直到已释放所有共享锁。

如果事务T 对数据A 加上共享锁后，则其他事务只能对A 再加共享锁，不能加排他锁。获得共享锁的事务只能读数据，不能修改数据。

```sql
SELECT ... LOCK IN SHARE MODE;
```

在查询语句后面增加 LOCK IN SHARE MODE，MySQ L会对查询结果中的每行都加共享锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请共享锁，否则会被阻塞。其他线程也可以读取使用了共享锁的表，而且这些线程读取的是同一个版本的数据。

排他锁又称写锁，如果事务T 对数据A 加上排他锁后，则其他事务不能再对A加任任何类型的锁。获得排他锁的事务既能读数据，又能修改数据。

```sql
SELECT ... FOR UPDATE;
```

在查询语句后面增加 FOR UPDATE，MySQL 会对查询命中的每条记录都加排他锁，当没有其他线程对查询结果集中的任何一行使用排他锁时，可以成功申请排他锁，否则会被阻塞。

#### 什么是意向锁？

MySQL 的 Innodb 引擎中，支持多种锁级别，包括了行级锁和表级锁。当多个事务想要访问一个共享资源的时候，如果每个事务都直接请求获取锁，那么就可能会导致互相阻塞，甚至导致死锁。

举个例子：

事务A 对表 Table1 中的一行加上了行级锁，这时候这行记录就只能读不能写了。事务B 申请对Table1 增加了表级锁，如果他申请成功了，那么他就可以修改表中的任意一行记录。这就发生了冲突。

那么，想要解决这个问题，就需要让事务B在对Table1增加表级锁的时候，先判断一下是不是有事务增加过行级锁。但是，事务B总不能遍历表中数据逐条判断是否有加锁吧？

所以，为了解决这个问题，MySQL 引入了意向锁机制。所以，意向锁是数据库管理系统中用于实现锁协议的一种锁机制，旨在处理不同锁粒度（如行锁和表锁）之间的并发性问题。（相同锁粒度（如多个行级锁）之间的并发性问题通过行级互斥锁解决。）

> 注意：
> 1、意向锁并不是直接锁定资源，而是为了通知其他事务，以防止它们在资源上设置不兼容的锁。
> 2、意向锁并不是直接由用户请求的，而是由 MySQL 管理的。

当一个事务请求获取一个行级锁或表级锁时，MySQL 会自动获取相应的表的意向锁。这样，其他事务请求获取表锁时，就可以先基于这个意向锁来发现是否有人加过锁，并根据该锁的类型（意向共享锁/意向排他锁）来判断自己是否可以获取锁。这样可以在不阻塞其他事务的情况下，为当前事务锁定资源。

意向锁有两种类型：意向共享锁和意向排他锁。

- 意向共享锁： 表示事务打算在资源上设置共享锁（读锁）。这通常用于表示事务计划读取资源，并不希望在读取时有其他事务设置排它锁。
- 意向排它锁： 表示事务打算在资源上设置排它锁（写锁）。这表示事务计划修改资源，并不希望有其他事务同时设置共享或排它锁。

意向锁是一个表级锁，并且他会在触发意向锁的事务提交或者回滚后释放。

#### 乐观锁与悲观锁如何实现？

在 MySQL 中，悲观锁是需要依靠数据库提供的锁机制实现的，在 InnoDB 引擎中，要使用悲观锁，需要先关闭 MySQL 数据库的自动提交属性，然后通过 `select ... for update` 来进行加锁。

在数据库中，悲观锁的流程如下：

- 在对记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。
- 如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。具体响应方式由开发者根据实际需要决定。
- 如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。
- 其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。

我们举一个简单的例子，如淘宝下单过程中扣减库存的需求说明一下如何使用悲观锁：

```sql
//0.开始事务
begin; 
//1.查询出商品信息
select quantity from items where id=1 for update;
//2.修改商品quantity为2
update items set quantity=2 where id = 1;
//3.提交事务
commit;
```

以上，在对 id = 1 的记录修改前，先通过 for update 的方式进行加锁，然后再进行修改。这就是比较典型的悲观锁策略。

如果以上修改库存的代码发生并发，同一时间只有一个线程可以开启事务并获得 id=1 的锁，其它的事务必须等本次事务提交之后才能执行。这样我们可以保证当前的数据不会被其它事务修改。

> 上面我们提到，使用 select…for update 会把数据给锁住，不过我们需要注意一些锁的级别，MySQL InnoDB 默认行级锁。行级锁都是基于索引的，如果一条 SQL 语句用不到索引的话，优化器在选择时候，如果发现锁表可能性能更好的话，有可能会直接锁表。

MySQL 中的乐观锁主要通过 CAS 的机制来实现，一般通过 version 版本号来实现。

CAS 是项乐观锁技术，当多个线程尝试使用 CAS 同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。

比如前面的扣减库存问题，通过乐观锁可以实现如下：

```sql
//查询出商品信息，quantity = 3
select quantity from items where id=1
//根据商品信息生成订单
//修改商品quantity为2
update items set quantity=2 where id=1 and quantity = 3;
```

以上，我们在更新之前，先查询一下库存表中当前库存数（quantity），然后在做 update 的时候，以库存数作为一个修改条件。当我们提交更新的时候，判断数据库表对应记录的当前库存数与第一次取出来的库存数进行比对，如果数据库表当前库存数与第一次取出来的库存数相等，则予以更新，否则认为是过期数据。

#### 悲观锁乐观锁如何选择？

在乐观锁与悲观锁的选择上面，主要看下两者的区别以及适用场景就可以了。

1、乐观锁并未真正加锁，效率高。适用于读操作频繁，写操作相对较少的场景。一旦锁的粒度掌握不好，更新失败的概率就会比较高，容易发生业务失败。 
2、悲观锁依赖数据库锁，效率低。更新失败的概率比较低。适用于写操作较为频繁，且并发写入的概率较高的场景。

#### Innodb 加索引，这个时候会锁表吗？

在 MySQL 5.6 之前，InnoDB 索引构建期间会对表进行排它锁定，这意味着在索引构建期间，其他会话不能读取或修改表中的任何数据。这将导致长时间阻塞和性能问题。

自 MySQL 5.6 开始，InnoDB 使用一种称为**“Online DDL”的技术**，允许在不阻塞其他会话的情况下创建或删除索引。Online DDL 针对不同的操作，有多种实现方式，包括COPY，INSTANT以及INPLACE

因为 DDL 有很多种操作，比如创建索引、增加字段、增加外键等，所以不同的操作支持的类型也不一样，具体支持方式可以在 mysql 官方可以看到：

https://dev.mysql.com/doc/refman/5.7/en/innodb-online-ddl-operations.html

#### Online DDL 原理

DDL，即 Data Defination Language，是用于定义数据库结构的操作。DDL 操作用于创建、修改和删除数据库中的表、索引、视图、约束等数据库对象，而不涉及实际数据的操作。以下是一些常见的DDL操作：

- CREATE  
- ALTER 
- DROP 
- TRUNCATE

与 DDL 相对的是 DML，即 Data Manipulation Language，用于操作数据。即包括我们常用的 INSERT、DELETE 和 UPDATE 等。

在 MySQL 5.6 之前，所有的 ALTER 操作其实是会阻塞 DML 操作的，如：添加/删除字段、添加/删除索引等，都是会锁表的。

但是在 MySQL 5.6 中引入了 Online DDL，OnLineDDL 是 MySQL5.6 提出的加速 DDL 方案，尽最大可能保证 DDL 期间不阻塞 DML 动作。但是需要注意，这里说的**尽最大可能意味着不是所有 DDL 语句都会使用 OnlineDDL 加速**。

Online DDL 的优点就是可以减少阻塞，是 MySQL 的一种内置优化手段，但是需要注意的是，DDL 在刚开始和快结束的时候，都需要获取 DML 锁，而在获取锁的时候如果有事务未提交，那么 DDL 就会因为加锁失败而进入阻塞状态，也会造成性能影响。

还有就是，如果 Online DDL 操作失败，其回滚操作可能成本较高。以及长时间运行的 Online DDL 操作可能导致主从同步滞后。

**DDL 算法 - COPY**

我们可以使用如下 SQL 指定 DDL 算法：

```sql
ALTER TABLE ping_ddl_test ADD PRIMARY KEY (id) ,ALGORITHM=INPLACE,LOCK=NONE
```

1. 新建一张临时表
2. 对原表加共享MDL锁，禁止原表的写，只允许查询操作
3. 逐行拷贝原表数据到临时表，且不进行排序
4. 拷贝完成后升级原表锁为排他MDL锁，禁止原表读写
5. 对临时表rename操作，创建索引，完成DDL操作

**DDL 算法 - INPLACE**

INPLACE 算法是 MySQL 5.5 中引入的，主要是为了优化索引的创建和删除过程的效率。INPLACE 算法的原理是可能地使用原地算法进行DDL操作，而不是重新创建或复制表。

1. 创建索引数据字典。
2. 对原表加共享MDL锁，禁止原表的写，只允许查询操作。
3. 根据聚集索引的顺序，查询表中的数据，并提取需要的索引列数据。将提取的索引数据进行排序，并插入到新的索引页中。
4. 等待当前表的所有只读事务提交。
5. 创建索引结束。

MySQL 中的 INPLACE 其实还可以分为以下两种算法：

- `inplace-no-rebuild`：对二级索引的增删改查、修改变长字段长度（如：varchar）、重命名列名都不需要重建原表
- `inplace-rebuild`：修改主键索引、增加删除列、修改字符集、创建全文索引等都需要重建原表。

**OnlineDDL 算法**

前面说过，ALGORITHM可以指定的DDL操作的算法，目前主要支持以下几种：

- COPY 算法
- INPLACE 算法
- INSTANT 算法：MySQL 8.0.12 引入的新算法，目前只支持添加列等少量操作，利用 8.0 新的表结构设计，可以直接修改表的元数据，省掉了重建原表的过程，极大的缩短了 DDL 语句的执行时间。其他类型的改表语句默认使用 inplace 算法。
  关于 instant 支持的场景可参考官方文档 [Online DDL Operations](https://dev.mysql.com/doc/refman/8.0/en/innodb-online-ddl-operations.html)
- DEFAULT：如果不指定 ALGORITHM，那么 MySQL 会自行选择默认算法，优先使用 INSTANT、其次是 INPLACE、再然后是 COPY

以下是 MySQL 官网上给出的 Online DDL 对索引操作的支持情况：

![OnlineDDL支持的索引操作](./images/OnlineDDL支持的索引操作.png)

**OnlineDDL 的原理**

以下是 OnlineDDL 的整体步骤，主要分为 Prepare 阶段、DDL 执行阶段以及 Commit 阶段。

Prepare 阶段：

1. 创建临时 frm 文件
2. 加 EXCLUSIVE-MDL 锁，禁止读写
3. 根据 alter 类型，确定执行方式（copy/online-rebuild/online-norebuild）。这里需要注意如果使用copy算法，就不是OnLineDDL了。
4. 更新数据字典的内存对象
5. 分配 row_log 对象，记录 OnlineDDL 过程中增量的 DML
6. 生成新的临时 idb 文件

Execute 阶段：

1. 降级EXCLUSIVE-MDL锁为SHARED-MDL锁，允许读写。
2. 扫描原表聚集索引的每一条记录。
3. 遍历新表的聚集索引和二级索引，逐一处理。
4. 根据原表中的记录构造对应的索引项。
5. 将构造的索引项插入 sort_buffer  块排序。
6. 将 sort_buffer 块更新到新表的索引上。
7. 记录 OnlineDDL 执行过程中产生的增量（oinline-rebuild）。
8. 重放 row_log  中的操作到新表的索引上（online-not-rebuild 数据是在原表上更新）。
9. 重放 row_log  中的 DML 操作到新表的数据行上。

Commit 阶段：

1. 升级到 EXCLUSIVE-MDL 锁，禁止读写。
2. 重做 row_log 中最后一部分增量。
3. 更新 innodb 的数据字典表。
4. 提交事务，写 redolog。
5. 修改统计信息。
6. rename 临时 ibd 文件，frm 文件。
7. 变更完成，释放 EXCLUSIVE-MDL 锁。

Prepare 阶段和 Commit 阶段虽然也加了 EXECLUSIVE-MDL 锁，但操作非常轻，所以耗时较低。Execute 阶段允许读写，通过 row_log 记录期间变更的数据记录，最后再应用 row_log 到新表中。最终实现 OnLineDDL 的效果。

#### InnoDB 中的索引类型？

InnoDB 存储引擎支持两种常见的索引数据结构：B+ 树索引、Hash 索引，其中 B+ 树索引是目前关系型数据库系统中最常见、最有效的索引。

数据库中的B+树索引分为聚集索引和非聚集索引。聚集索引就是按照每张表的主键构造一个 B+ 树，B+ 树的叶子节点中记录着表中一行记录的所有值。只要找到这个叶子节点也就得到了这条记录的所有值。非聚簇索引的叶节点中不包含行记录的所有值。只包含索引值和主键的值。

根据索引的唯一性，有可以把索引分为唯一索引和普通索引。唯一索引要求索引的列值必须唯一，不能重复。

另外，在MySQL 5.6中还增加了全文索引，5.7版本之后通过使用 ngram 插件开始支持中文。

#### InnoDB 为什么使用 B+ 树实现索引？

1. B+ 树是一棵平衡树，每个叶子节点到根节点的路径长度相同，查找效率较高；
2. B+ 树的所有关键字都在叶子节点上，因此范围查询时只需要遍历一遍叶子节点即可；
3. B+ 树的叶子节点都按照关键字大小顺序存放，因此可以快速地支持按照关键字大小进行排序；
4. B+ 树的非叶子节点不存储实际数据，因此可以存储更多的索引数据；
5. B+ 树的非叶子节点使用指针连接子节点，因此可以快速地支持范围查询和倒序查询。
6. B+ 树的叶子节点之间通过双向链表链接，方便进行范围查询。

那么，使用B+树实现索引，就有以下几个优点：

1. 支持范围查询，B+ 树在进行范围查找时，只需要从根节点一直遍历到叶子节点，因为数据都存储在叶子节点上，而且叶子节点之间有指针连接，可以很方便地进行范围查找。
2. 支持排序，B+ 树的叶子节点按照关键字顺序存储，可以快速支持排序操作，提高排序效率；
3. 存储更多的索引数据，因为它的非叶子节点只存储索引关键字，不存储实际数据，因此可以存储更多的索引数据；
4. 在节点分裂和合并时，IO 操作少。B+树的叶子节点的大小是固定的，而且节点的大小一般都会设置为一页的大小，这就使得节点分裂和合并时，IO 操作很少，只需读取和写入一页。
5. 有利于磁盘预读。由于 B+ 树的节点大小是固定的，因此可以很好地利用磁盘预读特性，一次性读取多个节点到内存中，这样可以减少IO操作次数，提高查询效率。
6. 有利于缓存。B+ 树的非叶子节点只存储指向子节点的指针，而不存储数据，这样可以使得缓存能够容纳更多的索引数据，从而提高缓存的命中率，加快查询速度。

**为什么不用红黑树或者B树？**

因为 B+ 树的特点是只有叶子节点存储数据，而非叶子节点不存储数据，并且节点大小固定，还有就是叶子结点之间通过双向链表链接的，所以，使用 B+ 树实现索引有很多好处，比如我们前面提到的支持范围查询、有利于磁盘预读、有利于优化排序等等。而这些是红黑树和 B 树做不到的。

**B+ 树索引和 Hash 索引有什么区别？**

B+ 树索引和哈希索引是常见的数据库索引结构，它们有以下几个主要区别：

B+ 树索引将索引列的值按照大小排序后存储，因此 B+ 树索引适合于范围查找和排序操作；而哈希索引是将索引列的值通过哈希函数计算后得到一个桶的编号，然后将桶内的记录保存在一个链表或者树结构中。因此，哈希索引适合于等值查询，但不适合范围查询和排序操作。

B+ 树索引在插入和删除数据时需要调整索引结构，这个过程可能会涉及到页分裂和页合并等操作，因此B+ 树索引的维护成本比较高；而哈希索引在插入和删除数据时只需要计算哈希值并插入或删除链表中的记录，因此哈希索引的维护成本相对较低。

B+ 树索引在磁盘上是有序存储的，因此在进行区间查询时可以利用磁盘预读的优势提高查询效率；而哈希索引在磁盘上是无序存储的，因此在进行区间查询时可能会需要随机访问磁盘，导致查询效率降低。

B+ 树索引在节点中存储多个键值对，因此可以充分利用磁盘块的空间，提高空间利用率；而哈希索引由于需要存储哈希值和指针，因此空间利用率相对较低。

#### MySQL 是如何保证唯一性索引的唯一性的？

MySQL 通常使用 B 树（或变种如 B+ 树）作为唯一性索引的数据结构。这种结构允许高效的数据检索和插入操作。当插入一个新行或更新现有行的索引列时，MySQL 首先在索引中检查是否已经存在相同的键值。如果发现索引列的新值已经存在于唯一性索引中，MySQL 将阻止该插入或更新操作，并返回一个错误。

在支持事务的存储引擎（如 InnoDB）中，事务机制和锁定协议帮助维护索引的唯一性。当一个事务正在修改索引列时，其他事务对相同键值的修改会被适当地阻塞，直到第一个事务提交或回滚，确保了数据的一致性和唯一性。

并且，在实际写入数据到磁盘之前，MySQL 也会执行约束检查，确保不会违反唯一性约束。

#### 唯一索引允许 NULL 值吗？

唯一索引在 MySQL 中可以允许 NULL 值的，但是这些NULL的表现是未知的，未知就是他们不相等，但是也不能说他们不等。

并且，InnoDB 存储引擎在 MySQ L中支持在唯一索引中有多个 NULL 值。这是因为在 MySQL 中，NULL 被认为是“未知”的，**每个 NULL 值都被视为互不相同**。因此，即使一个列被定义了唯一索引，它也可以包含多个 NULL 值。

#### 唯一性索引查询更快吗？

在数据库中，通过唯一性索引来创建唯一性约束，可以保证表中指定列的值唯一，避免数据重复和错误插入。

唯一性索引查询通常会比非唯一性索引查询更快，因为唯一性索引能够快速定位到唯一的记录，而非唯一性索引则需要扫描整个索引并匹配符合条件的记录。

在应用中，如果我们能够设计合适的唯一性索引，也可以有效地提升查询性能和数据质量。

#### 唯一性索引有什么缺点吗？

没有银弹，所以一定有缺点。

首先，唯一性索引需要保证索引列的唯一性，因此在插入数据时需要检查是否存在相同的索引值，这会对插入性能产生一定的影响。

如果需要更新唯一性索引列的值，需要先删除旧记录，再插入新记录，这会对更新操作的成本产生影响。

#### 什么是聚簇索引和非聚簇索引？

聚簇索引（Clustered Index）和非聚簇索引（Non-clustered Index）是数据库中的两种索引类型，它们在组织和存储数据时有不同的方式。

聚簇索引，简单点理解就是将数据与索引放到了一起，找到索引也就找到了数据。也就是说，对于聚簇索引来说，他的非叶子节点上存储的是索引字段的值，而他的叶子节点上存储的是这条记录的整行数据。

![聚簇索引](./images/聚簇索引.png)

**在 InnoDB 中，聚簇索引（Clustered Index）指的是按照每张表的主键构建的一种索引方式**，它是将表数据按照主键的顺序存储在磁盘上的一种方式。这种索引方式保证了行的物理存储顺序与主键的逻辑顺序相同，因此查找聚簇索引的速度非常快。

非聚簇索引，就是将数据与索引分开存储，叶子节点包含索引字段值及指向数据页数据行的逻辑指针。

![非聚簇索引](./images/非聚簇索引.png)

**在 Innodb 中，非聚簇索引（Non-clustered Index）是指根据非主键字段创建的索引，也就是通常所说的二级索引**。它不影响表中数据的物理存储顺序，而是单独创建一张索引表，用于存储索引列和对应行的指针。

在 InnoDB 中，主键索引就是聚簇索引，而非主键索引，就是非聚簇索引，所以在 InnoDB 中：

- 对于聚簇索引来说，他的非叶子节点上存储的是索引值，而它的叶子节点上存储的是整行记录。
- 对于非聚簇索引来说，他的非叶子节点上存储的都是索引值，而它的叶子节点上存储的是主键的值+索引值。

所以，通过非聚簇索引的查询，需要进行一次回表，就是先查到主键ID，在通过ID 查询所需字段。

> 没有创建主键怎么办？
>
> 其实，数据库中的每行记录中，除了保存了我们自己定义的一些字段以外，还有一些重要的 `db_row_id` 字段，其实他就是一个数据库帮我添加的隐藏主键，如果我们没有给这个表创建主键，会选择一个不为空的唯一索引来作为聚簇索引，但是如果没有合适的唯一索引，那么会以这个隐藏主键来创建聚簇索引。

#### 什么是回表，怎么减少回表的次数？

在 InnoDB 里，索引 B+ Tree 的叶子节点存储了整行数据的是主键索引，也被称之为聚簇索引。而索引 B+ Tree 的叶子节点存储了主键的值的是非主键索引，也被称之为非聚簇索引。

在存储的数据方面，主键（聚簇）索引的B+树的叶子节点直接就是我们要查询的整行数据了。而非主键（非聚簇）索引的叶子节点是主键的值。

那么，**当我们根据非聚簇索引查询的时候，会先通过非聚簇索引查到主键的值，之后，还需要再通过主键的值再进行一次查询才能得到我们要查询的数据。而这个过程就叫做回表。**

所以，在InnoDB 中，**使用主键查询**的时候，是效率更高的， 因为这个过程不需要回表。另外，依赖**覆盖索引、索引下推**等技术，我们也可以通过优化索引结构以及SQL语句减少回表的次数。

#### 什么是索引覆盖、索引下推？

> 覆盖索引

覆盖索引（covering index）指一个查询语句的执行只用从索引中就能够取得，不必从数据表中读取。也可以称之为实现了索引覆盖。 

当一条查询语句符合覆盖索引条件时，MySQL 只需要通过索引就可以返回查询所需要的数据，这样避免了查到索引后再返回表操作，减少 I/O 提高效率。 

如，表 `covering_index_sample` 中有一个普通索引 `idx_key1_key2(key1,key2)`。

当我们通过 SQL 语句：

`select key2 from covering_index_sample where key1 = ‘keytest’;`

的时候，就可以通过覆盖索引查询，无需回表。

但是以下 SQL，因为不符合最左前缀匹配，虽然是索引覆盖，但是也无法用到索引（会扫描索引树）：

`select key1 from covering_index_sample where key2 = ‘keytest’;`

但是如果 SQL 中查询的信息不包含在联合索引中，那么就不会走索引覆盖。如：

`select key2,key3 from covering_index_sample where key1 = ‘keytest’;`

> 索引下推

索引下推是 MySQL 5.6 引入了一种优化技术，默认开启，使用 `SET optimizer_switch = ‘index_condition_pushdown=off’;` 可以将其关闭。

官方文档中给的例子和解释如下： people表中（zipcode，lastname，firstname）构成一个索引
`SELECT * FROM people WHERE zipcode=’95054′ AND lastname LIKE ‘%etrunia%’ AND address LIKE ‘%Main Street%’;`

如果没有使用索引下推技术，则 MySQL会通过 `zipcode=’95054’` 从存储引擎中查询对应的数据，返回到 MySQL 服务端，然后 MySQL 服务端基于 `lastname LIKE ‘%etrunia%’` 和 `address LIKE ‘%Main Street%’` 来判断数据是否符合条件。 

如果使用了索引下推技术，则 MYSQL 首先会返回符合 `zipcode=’95054’` 的索引，然后根据 `lastname LIKE ‘%etrunia%’` 来判断索引是否符合条件。

如果符合条件，则根据该索引来定位对应的数据，如果不符合，则直接 reject 掉。 有了索引下推优化，可以在有 like 条件查询的情况下，减少回表次数。

当一条 SQL 使用到索引下推时，explain 的执行计划中的 extra 字段中内容为：`Using index condition`

> 索引下推不止 like

上面的例子中，提到了 like，包括 MySQL 官网中也只提到了 like，但是其实不止有 Like。因为我认为索引下推其实是解决索引失效带来的效率低的问题的一种手段。


所以当联合索引中，某个非前导列因为索引失效而要进行扫表并回表时，就可以进行索引下推优化了。

如，有 a,b 联合索引，类型都是 varchar，以下 SQL 也可以用到索引下推：

`select d from t2 where a = "ni" and b = 1;` 

因为 b 字段因为类型不匹配导致索引失效了，但是通过下推优化其实是可以减少回表的次数的。

#### 设计索引的时候有哪些原则？

1. **考虑查询的频率和效率：**在决定创建索引之前，需要分析查询频率和效率。对于频繁查询的列，可以创建索引来加速查询，但对于不经常查询或者数据量较少的列，可以不创建索引。
2. **选择适合的索引类型：**MySQL提供了多种索引类型，如 B+Tree 索引、哈希索引和全文索引等。不同类型的索引适用于不同的查询操作，需要根据实际情况选择适合的索引类型。
3. **考虑区分度：**尽量不要选择区分度不高的字段作为索引，比如性别。但是也并不绝对，对于一些数据倾斜比较严重的字段，虽然区分度不高，但是如果有索引，查询占比少的数据时效率也会提升。
4. **考虑联合索引：**联合索引是将多个列组合在一起创建的索引。当多个列一起被频繁查询时，可以考虑创建联合索引。
5. **考虑索引覆盖：**联合索引可以通过索引覆盖而避免回表查询，可以大大提升效率，对于频繁的查询，可以考虑将 select 后面的字段和 where 后面的条件放在一起创建联合索引。
6. **避免创建过多的索引：**创建过多的索引会占用大量的磁盘空间，影响写入性能。并且在数据新增和删除时也需要对索引进行维护。所以在创建索引时，需要仔细考虑需要索引的列，避免创建过多的索引。
7. **避免使用过长的索引：**索引列的长度越长，索引效率越低。在创建索引时，需要选择长度合适的列作为索引列。对于文本列，可以使用前缀索引来减少索引大小。
8. **合适的索引长度：**虽然索引不建议太长，但是也要合理设置，如果设置的太短，比如身份证号，但是只把前面6位作为索引，那么可能会导致大量锁冲突。
9. **执行计划分析：**多用执行计划分析，因为随着数据库的数据量变化、索引数量变化，最终使用的索引可能也不太一样，所以需要经常查看索引是否有使用对。

#### MySQL索引一定遵循最左前缀匹配吗？

因为索引底层是一个 B+ 树，如果是联合索引的话，在构造 B+ 树的时候，会先按照左边的 key 进行排序，左边的 key 相同时再依次按照右边的 key 排序。

所以，在通过索引查询的时候，也需要遵守最左前缀匹配的原则，也就是需要从联合索引的最左边开始进行匹配，这时候就要求查询语句的 where 条件中，包含最左边的索引的值。这也就是最左前缀匹配。

MySQL 一定是遵循最左前缀匹配的，这句话在以前是正确的，没有任何毛病。但是在 MySQL 8.0 中，就不一定了。因为 8.0.13 中引入了索引跳跃扫描。

#### 索引跳跃扫描

MySQL 8.0.13 版本中，对于 range 查询（什么是 range 后面会提到)，引入了索引跳跃扫描（Index Skip Scan）优化，支持不符合组合索引最左前缀原则条件下的 SQL，依然能够使用组合索引，减少不必要的扫描。

通过一个例子给大家解释一下，首先有下面这样一张表（参考了MySQL官网的例子，但是我做了些改动和优化）：

```sql
CREATE TABLE t1 (f1 INT NOT NULL, f2 INT NOT NULL);
CREATE INDEX idx_t on t1(f1,f2);
INSERT INTO t1 VALUES
  (1,1), (1,2), (1,3), (1,4), (1,5),
  (2,1), (2,2), (2,3), (2,4), (2,5);
INSERT INTO t1 SELECT f1, f2 + 5 FROM t1;
INSERT INTO t1 SELECT f1, f2 + 10 FROM t1;
INSERT INTO t1 SELECT f1, f2 + 20 FROM t1;
INSERT INTO t1 SELECT f1, f2 + 40 FROM t1;
```

通过上面的 SQL，先创建一张 t1 表，并把 f1,f2 两个字段设置为联合索引。之后再向其中插入一些记录。

分别在 MySQL 5.7.9 和 MySQL 8.0.30 上执行 `EXPLAIN SELECT f1, f2 FROM t1 WHERE f2 = 40;`

> MySQL 5.7中，type = index，rows=160，extra=Using where;Using index
>
> MySQL 8.0中，type = range，rows=16，extra=Using where;Using index for skip scan

这里面的 type 指的是扫描方式，range 表示的是范围扫描，index 表示的是索引树扫描，通常情况下，range 要比 index 快得多。

从 rows 上也能看得出来，使用 index 的扫描方式共扫描了 160 行，而使用 range 的扫描方式只扫描了 16 行。

接着，重点来了，那就是为啥 MySQL 8.0 中的扫描方式可以更快呢？主要是因为 `Using index for skip scan` 表示他用到了索引跳跃扫描的技术。

也就是说，虽然我们的 SQL 中，没有遵循最左前缀原则，只使用了 f2 作为查询条件，但是经过 MySQL 8.0 的优化以后，还是通过索引跳跃扫描的方式用到了索引了。

**优化原理**

那么他是怎么优化的呢？

在 MySQL 8.0.13 及以后的版本中，`SELECT f1, f2 FROM t1 WHERE f2 = 40;` SQL执行过程如下：

- 获取 f1 字段第一个唯一值，也就是 f1=1
- 构造 `f1=1 and f2 = 40`，进行范围查询
- 获取 f1 字段第二个唯一值，也就是 f1=2
- 构造 `f1=2 and f2 = 40`，进行范围查询
- 一直扫描完 f1 字段所有的唯一值，最后将结果合并返回

也就是说，最终执行的 SQL 语句是像下面这样的：

```sql
SELECT f1, f2 FROM t1 WHERE f1 =1 and f2 = 40
UNION
SELECT f1, f2 FROM t1 WHERE f1 =2 and f2 = 40;
```

即，MySQL 的优化器帮我们把联合索引中的f1字段作为查询条件进行查询了。

**限制条件**

在知道了索引跳跃扫描的执行过程之后，很多聪明的读者其实就会发现，这种查询优化比较适合于f1的取值范围比较少，区分度不高的情况（如性别），一旦f1的区分度特别高的话（如出生年月日），这种查询可能会更慢。

所以，真正要不要走索引跳跃扫描，还是要经过 MySQL 的优化器进行成本预估之后做决定的。

所以，这种优化一般用于那种联合索引中第一个字段区分度不高的情况。但是话又说回来了，我们一般不太会把区分度不高的字段放在联合索引的左边，不过事无绝对，既然 MySQL 给了一个优化的方案，就说明还是有这样的诉求的。

但是，我们不能依赖他这个优化，建立索引的时候，还是优先把区分度高的，查询频繁的字段放到联合索引的左边。

除此之外，在 MySQL 官网中，还提到索引跳跃扫描还有一些其他的限制条件：

- 表T 至少有一个联合索引，但是对于联合索引 `(A,B,C,D)` 来说，A 和 D 可以是空的，但 B 和 C 必须是非空的。
- 查询必须只能依赖一张表，不能多表 join
- 查询中不能使用 GROUP BY 或 DISTINCT 语句
- 查询的字段必须是索引中的列

#### MySQL的主键一定是自增的吗？

不是的，主键是可以自己选择的，我们可以选择任意一种数据类型作为主键。

但是一般都是单独创建一个自增字段作为主键，主要能带来以下几个好处：

1. 索引大小更小：使用自增主键可以确保主键的递增性，使得新插入的数据都会在索引的末尾处，减少了数据页的分裂和页分裂导致的IO操作，使得索引大小更小，查询速度更快。
2. 索引顺序和插入顺序相同：使用自增主键可以保证索引顺序和插入顺序相同，减少了插入新数据时索引的重新排序，提高了插入速度。
3. 安全性：使用自增主键可以避免主键重复的情况，确保数据完整性和唯一性。
4. 减少页分裂（合并）及内存碎片。

另外，即使我们用了自增字段作为主键，最后体现在数据库上，字段也可能不是连续自增的，比如做了某一次插入失败但是主键ID被占用了，比如数据做了删除，比如做了回滚、比如做了数据订正等等。

#### 介绍一下 InnoDB 的数据页，和 B+ 树的关系是什么？

InnoDB 的数据页是 InnoDB 存储引擎中用于存储数据的基本单位。它是磁盘上的一个连续区域，通常大小为 16KB 当然，也可以通过配置进行调整。16KB 就意味着 Innodb 的每次读写都是以 16KB 为单位的，一次从磁盘到内存的读取的最小是 16KB，一次从内存到磁盘的持久化也是最小 16KB。

B+ 树的每个节点都对应着一个数据页，包括根节点、非叶子节点和叶子节点。B+ 树通过节点之间的指针连接了不同层级的数据页，从而构建了一个有序的索引结构。

![InnoDB数据页](./images/InnoDB数据页.png)

通过 B+ 树的搜索过程，可以从根节点开始逐层遍历，最终到达叶子节点，找到所需的数据行。

所以，数据页是存储数据行的实际物理空间，以页为单位进行磁盘读写操作。B+ 树通过节点和指针的组织，构建了一个层次结构的索引，用于快速定位和访问数据行。

B+ 树的非叶子节点对应着数据页，其中存储着主键+指向子节点（即其他数据页）的指针。B+ 树的叶子节点包含实际的数据行，每个数据行存储在一个数据页中。

通过这种方式，InnoDB 利用 B+ 树和数据页的组合，实现了高效的数据存储和检索。B+ 树提供了快速的索引查找能力，而数据页提供了实际存储和管理数据行的机制。它们相互配合，使得 InnoDB 能够处理大规模数据的高效访问。

**数据页的构成**

![数据页的构成](./images/数据页的构成.png)

#### 什么是 InnoDB 的页分裂和页合并

InnoDB 的数据页是 InnoDB 存储引擎中用于存储数据的基本单位，通常大小为 16KB。B+ 树的每个节点都对应着一个数据页，包括根节点、非叶子节点和叶子节点。B+ 树通过节点之间的指针连接了不同层级的数据页，从而构建了一个有序的索引结构。

我们都是知道，B+ 树是按照索引字段建立的，并且在 B+ 树中是有序的，假如有下面一个索引的树结构，其中的索引字段的值并不连续。

![索引树结构](./images/索引树结构.png)

假如，现在我们插入一个新的一条记录，他的索引值是3，那么他就要按照顺序插入到页20中，在索引值为1,2的记录的后面。而如果这个索引页已经满了，那么就需要触发一次页分裂。

> 页分裂是指将该页面中的一部分索引记录移动到一个新的页面中，从而为新记录腾出空间。这样可以保持 B+ 树的平衡和性能。

以下，就是一次页分裂的过程：

![页分裂过程](./images/页分裂过程.png)

那么，当我们向 Innodb 中添加数据的时候，如果索引是随机无序的，那么就会导致页分裂。而且分裂这个动作还可能会引起连锁反应，从叶子节点沿着树结构一路分裂到根节点。

有分裂，就会有合并。在 InnoDB 中，当索引页面中的索引记录删除后，页面可能会变得过于稀疏。这时，为了节省空间和提高性能，可能会触发页合并操作。

> 页合并是指将两个相邻的索引页面合并成一个更大的页面，减少 B+ 树的层级，从而提高查询性能。

![页合并过程](./images/页合并过程.png)

**页分裂（合并）的危害**

首先，页分裂和合并是涉及大量数据移动和重组的操作。频繁进行这些操作会增加数据库的 I/O 负担和 CPU 消耗，影响数据库的整体性能。

分裂和合并可能导致 B+ 树索引结构频繁调整，这个过程也会影响插入及删除操作的性能。

频繁的页分裂和合并可能会导致磁盘上存在较多的空间碎片，新分出的一个页一般会有很多空闲空间，使得数据库表占用更多的磁盘空间，而导致浪费。

**如何避免页分裂**

> 【使用 varchar 会导致页分裂】
>
> varchar(100) 由于没有提前分配存储空间，因为表中数据刚开始插入时，可变长度字段值都是根据实际长度存储下来的，且行与行之间数据也是紧密连续存放在文件地址中的。那么现在值变长了，原来的位置无法扩展出新的空间出来，所以无法覆盖存放到原来的位置上。此时 MySQL 就会使用页分裂的方法扩展字段变长的空间。
>
> 使用 varchar 可能会产生内存碎片、varchar 会额外需要 1 到 2 个字节存储长度信息、以及 update 语句可能会导致页分裂。
>
> 【使用 UUID 作为主键会导致页分裂】

所以，尽量选择使用自增的字段作为索引，尤其是主键索引，这样可以很大程度的避免页分裂。

如果要插入大量数据，尽量使用批量插入的方式，而不是逐条插入。这样可以减少页分裂的次数。

频繁删除操作可能导致页面过于稀疏，从而触发叶合并。所以，一般建议使用逻辑删除而不是物理删除。

逻辑删除：即在记录中添加一个标记来表示记录是否被删除(`deleted  = 0/1`)，而不是真正地从数据库中删除记录。

我们当然还可以根据实际情况，适当调整 InnoDB 的配置参数，如页大小、填充因子、叶子页合并的阈值等，以优化数据库性能。

#### uuid 和自增id 做主键哪个好，为什么？

UUID 和自增主键ID 是作为数据库主键比较常用的方式。使用这两个做主键各自都有优缺点。

**UUID**

UUID 是一个 128 位长的唯一标识符，通常以字符串形式表示。它可以使用不同的算法生成，比如基于时间戳的 UUID（version 1）和随机数生成的 UUID（version 4）等。

他的优点是：

- 全局唯一：使用不同的算法生成，几乎可以保证在全球范围内唯一，避免了多台机器之间主键冲突的问题。
- 不可预测性：随机生成的 UUID 很难被猜测出来，对于需要保密性的应用场景较为适用。
- 分布式：由于可以在不同的机器上生成 UUID，因此可以用于分布式系统中。

UUID当做主键ID的话的缺点同样也有，如：

- 存储空间比较大：UUID 通常以字符串形式存储，占用的存储空间比较大。
- 不适合范围查询：因为不是自增的，所以在做范围查询的时候是不支持的。（后插入的UUID在排序时可能比前面的要小，在做范围查询的时候可能会出现数据重复或者漏数据的情况）
- 不方便展示：主键ID有的时候会需要在系统间、或者前台页面展示，如果是UUID的话，就因为比较长、并且没有任何业务含义，所以不方便展示。
- 查询效率低：
  - 在UUID列上创建索引，因为他很长，所以索引的大小会变得非常大。大的索引会占用更多的磁盘空间，导致缓存命中率下降，进而增加了磁盘I/O的需求。此外，大的索引还会导致查询时的内存开销增加。
  - 当使用UUID进行排序时，新的UUID值通常会插入到叶子节点的中间位置。这可能导致B+树的分裂和平衡操作频繁进行，从而增加了写入的开销。每次分裂或平衡都涉及到数据的重新排序和移动，这会影响查询的性能。

**自增ID**

在 MySQL 中，可以通过设置 AUTO_INCREMENT 属性实现 ID 的自增长，通常可以用来作为主键 ID。

使用自增ID做主键的好处是：

- 存储空间：ID 是数字，所以占用的位数要比 UUID 小多了，所以在存储空间上也节省很多。
- 查询效率：ID 是递增的，因此在使用 B+Tree 索引时，查询效率较高。
- 方便展示：因为 ID 比较短，方便展示。
- 分页方便：因为 ID 是连续自增的，所以在分页的时候，可以通过ID解决深度分页的问题。

但是，使用自增主键做主键 ID 也存在一定的问题：

- 分库分表：当我们做分库分表的时候，就没办法依赖一张表的自增主键来做主键 ID 了，这样就会发生重复导致冲突的问题
- 可预测：因为 ID 是顺序自增的，所以是可以预测的，这就给系统带来了一定的安全风险。
- 可能用尽：自增 id 的话可能是 int、bigint 等，但是他们都是有范围的，有可能会用尽。

#### order by 是怎么实现的？

order by 是做排序的，具体怎么排取决于优化器的选择，如果优化器认为走索引更快，那么就会用索引排序，否则，就会使用 filesort (执行计划中extra中提示：using filesort），但是能走索引排序的情况并不多，并且确定性也没有那么强，很多时候，还是走的 filesort。

filesort 这种排序方式中，如果需要排序的内容比较少，就会基于内存中的 sort_buffer，否则就需要使用临时文件进行排序了。并且在实际排序过程中，如果字段长度并不是特别长，那么就会使用全字段排序的方式直接在 sort_buffer 中排序后返回结果集。如果字段长度特别长，那么就可能基于空间考虑，采用 row_id 排序，这样就会在排序后进行二次回表后返回结果集。

**索引排序**

我们都知道，索引是天然有序的，所以当我们在使用 order by 的时候，如果能借助索引，那么效率一定是最高的。

并且 MySQL 确实也可以基于索引进行 order by 的查询，但是这个过程是否一定用索引，完全取决于优化器的选择。

```sql
 CREATE TABLE `t2` (          
  `id` INT(11),
  `a` varchar(64) NOT NULL,                                                                                                                                                          
  `b` varchar(64) NOT NULL,                                                                                                                                                          
  `c` varchar(64) NOT NULL,                                                                                                                                                          
  `d` varchar(64) NOT NULL,                                                                                                                                                          
  `f` varchar(64) DEFAULT NULL,    
  PRIMARY KEY(id),
  UNIQUE KEY `f` (`f`),                                                                                                                                                              
  KEY `idx_abc` (`a`,`b`,`c`)         
  KEY `idx_a` (`a`)    
) ENGINE=InnoDB DEFAULT CHARSET=latin1
```

假设有以上这样一张表，在排序时，可能出现的情况如下（因为有优化器的干预，以下结果并不一定可以100%复现。我自己实验的时候是可以的，我的环境是MySQL 5.7，）：

```sql
select * from t2 order by a;

-- 不走索引，使用filesort（后面介绍啥是filesort）


select * from t2 order by a limit 100;

-- 走索引


select a,b from t2 order by a limit 100;

-- 走索引


select a,b,c from t2 order by a;

-- 走索引


select a,b,c,d from t2 order by a;

-- 不走索引，使用filesort


select a,b,c,d from t2 where a = "ping" order by b;

-- 走索引


select a,b,c,d from t2 where b = "ping" order by a;

-- 不走索引，使用filesort
```

也就是说，当我们使用索引字段进行排序的时候，优化器会根据成本评估选择是否通过索引进行排序。经过我的不断验证，以下几种情况，走索引的概率很高：

- 查询的字段和 order by 的字段组成了一个联合索引，并且查询条件符合最左前缀匹配，查询可以用到索引覆盖。如 `select a,b,c from t2 order by a;`
- 查询条件中有 limit，并且 limit 的数量并不高。（我测试的表数据量 80 万，在 limit 超过 2W 多的时候就不走索引了），如 `order by a limit 100`
- 虽然没有遵循最左前缀匹配，但是前导列通过常量进行查询，如 `where a = "ping" order by b`

**filesort 排序**

如果不能使用或者优化器认为索引排序效率不高时， MySQL 会执行 filesort 操作以读取表中的行并对它们进行排序。

在进行排序时，MySQL 会给每个线程分配一块内存用于排序，称为 `sort_buffer`，它的大小是由 `sort_buffer_size` 控制的。

而根据 `sort_buffer_size` 的大小不同，会在不同的地方进行排序操作：
●
如果要排序的数据量小于 `sort_buffer_size`，那么排序就在内存中完成。
●
如果排序数据量大于 `sort_buffer_size`，则需要利用磁盘临时文件辅助排序。

临时文件排序采用归并排序的算法，首先会把需要排序的数据拆分到多个临时文件里同步进行排序操作，然后把多个排好序的文件合并成一个结果集返回给客户端。
在磁盘上的临时文件里排序相对于在内存中的 `sort buffer` 里排序来说，会慢很多。

除了 `sort_buffer_size` 参数以外，影响排序的算法的还有一个关键参数：`max_length_for_sort_data`

`max_length_for_sort_data` 是 MySQL 中控制 `<用于排序的行数据的长度>` 的一个参数，默认值是 1024 字节。如果单行的长度超过这个值，MySQL 就认为单行太大，那么就会采用 `rowid` 排序，否则就进行全字段排序。

**全字段排序**

所谓全字段排序，就是将要查询的所有字段都放到 `sort_buffer` 中，然后再根据排序字段进行排序，排好之后直接把结果集返回给客户端。

假如我们有如下查询 SQL：

```sql
select a,d,f from t2 where a = "ping" order by d;
```

因为这里涉及到的字段有 a,d,f 三个，那么就会把符合 where 条件的所有数据的 a,d,f 字段都放到 `sort_buffer` 中，然后再在 `sort_buffer` 中根据 d 进行排序，排好之后返回给客户端。大致过程如下：

1. 从索引 a 中取出 a = "ping" 的第一条记录的主键ID
2. 根据主键 ID 回表，取出 a,d,f 三个字段，存入 sort_buffer
3. 继续查询下一个符合 a = "ping" 的记录，重读第1-2步骤
4. 在 sort_buffer 中，根据 d 进行排序
5. 将排序后的结果集返回给客户端

![order怎么实现的-全字段排序](./images/order怎么实现的-全字段排序.png)

以上过程中，如果数据 sort buffer 放不下了，就会采用临时文件，然后再对临时文件进行归并排序。

全字段排序的好处就是只对原表进行了一次回表查询（每条记录只需要回表一次），之后的排序好以后就可以直接把需要的字段返回了。所以他的效率比较高。但是他的缺点就是，如果要查询的字段比较多，那么就会比较耗费 sort_buffer 的空间，使得空间中能存储的数据很少。那么如果要排序的数据量变大，就会要用到临时文件，导致整体的性能下降。

那么，为了避免这个问题，也可以通过 row_id 排序的方式。

**row_id 排序**

这个也比较容易理解，就是说我们在构建 `sort_buffer` 的时候，不要把所有的要查询字段都放进去，只把排序字段这主键放进去就行了。

`select a,d,f from t2 where a = "Hollis" order by d;`

比如这个 SQL，那么只需要把 d 和 id 放到 `sort_buffer` 中，先按照 d 进行排序。排好之后，就在根据 id，把对应的 a,d,f 几个字段都查询出来，返回给客户端即可。大致过程如下：

1. 从索引 a 中取出 a = "ping" 的第一条记录的主键 ID
2. 根据主键 ID 回表，取出 d 这一个字段，存入 sort_buffer
3. 继续查询下一个符合 a = "ping" 的记录，重读第 1-2 步骤
4. 在 sort_buffer 中，根据 d 进行排序
5. 再根据 ID，回表查询出 a,d,f 几个字段
6. 将结果集返回给客户端

> 【如何选择】
>
> 其实，row_id 是 MySQL 的一种优化算法，他会优先考虑使用全字段排序，只有在他认为字段长度过长，可能会影响效率时，采用 row_id 的方式排序。并且，能用 sort_buffer 搞定的情况，MySQL 就不会采用临时文件。
>
> 总之就是，速度优先，内存优先、一次回表优先。

#### count(1)、count(*) 与 count(列名) 的区别

#### limit 0,100 和 limit 10000000,100 一样吗？

#### SQL 语句如何实现 insertOrUpdate 的功能？

#### binlog、redolog 和 undolog 区别？

#### 用了索引还是很慢，可能是什么原因？

#### SQL 执行计划分析的时候，要关注哪些信息？

#### 是否支持 emoji 表情存储，如果不支持，如何操作？

#### 如何优化一个大规模的数据库系统？

#### MySQL 只操作同一条记录，也会发生死锁吗？

#### 数据库死锁如何解决？

#### 索引失效的问题如何排查？

#### 如何进行 SQL 调优？

#### 区分度不高的字段建索引一定没用吗？

#### 慢 SQL 的问题如何排查？

慢查询是指数据库中查询时间超过指定阈值的 SQL，这个阈值根据不同的业务来说一般是不一样的，在我们内部，这个阈值是1s，也就是说，如果一条 SQL 执行超过 1 秒钟，就认为是一个慢 SQL。

慢 SQL 的问题排查一般分为几个步骤。

**发现问题**

一般来说，慢 SQL 的问题，是比较容易能够发现的。首先如果有很成熟的监控体系的话，会把慢 SQL 进行统计，然后以报警的形式推送出来。

如果用了一些数据库的中间件，他们也会有慢 SQL 的日志，如 TDDL：

```sql
Cause: ERR-CODE: [TDDL-4202][ERR_SQL_QUERY_TIMEOUT] Slow query leads to a timeout exception, please contact DBA to check slow sql. SocketTimout:12000 ms, 
```

如果中间件也没用，那么数据库自己也是可以配置慢 SQL 日志的，配置方式如下：

1、找到 MySQL 的配置文件 my.cnf（或 my.ini，取决于操作系统），通常位于 MySQL 安装目录下的 etc 或 conf 文件夹。
2、在配置文件中启用慢查询日志：找到或添加以下配置项，并取消注释（如果有注释），确保以下行存在或添加到配置文件中：

```sql
slow_query_log = 1
slow_query_log_file = /path/to/slow-query.log
long_query_time = 1
```

3、重启 MySQL 服务：保存配置文件并重新启动 MySQL 服务，使配置生效。

配置生效后，如果有 SQL 的执行时长超过 `long_query_time` 配置的时间阈值，那么就会打印慢 SQL 日志。

4、查看慢查询日志：使用文本编辑器打开慢查询日志文件，路径为在配置文件中指定的路径。例如，在 Linux 上可以使用以下命令：

```sql
vim /path/to/slow-query.log
```

如果有慢 SQL，内容如下：

```sql
# Time: 2023-06-04T12:00:00.123456Z
# User@Host: hollis[192.168.0.1]:3306
# Query_time: 2.345678  Lock_time: 0.012345 Rows_sent: 10  Rows_examined: 100
SET timestamp=1650000000;
SELECT * FROM orders WHERE status = 'pending' ORDER BY gmt_created DESC;
```

**定位问题**

在如上的各种监控、报警以及日志中，我们就可以找到对应的慢SQL的具体SQL了，然后就可以进一步分析为什么这个 SQL 是慢 SQL 了，主要就是排查一下他慢在哪里。

一般来说，导致一个 SQL 慢的常见原因有以下几个：

1、没有索引
2、用错索引（没有遵守最左前缀、索引区分度不高）
3、查询字段太多
4、回表次数多
5、多表join
6、深度分页
7、...其他...

一般来说，大多数情况，是可以通过执行计划分析出一条 SQL 的慢的原因的，大部分来说，主要是索引的问题和 join 的问题了。

**解决问题**

在定位到问题之后，就是解决了

其实最难的不是解决问题，而是定位问题，因为一旦一个问题被定位到了，解决起来都比较容易。缺索引就加索引，join 太多就拆分就好了。这里就不展开说了。

#### MySQL 主从复制的过程

#### 介绍一下 InnoDB 的数据页，和 B+ 树的关系是什么？

#### MySQL 的驱动表是什么？MySQL 怎么选的？

#### MySQL 的 Hash Join 是什么？

#### MySQL 执行大事务会存在什么问题？

#### MySQL 怎么做热点数据高效更新？

#### SQL 中 PK、UK、CK、FK、DF 是什么意思？

#### 什么是 buffer pool？

#### buffer pool 的读写过程是怎么样的？

#### MySQL 自增主键用完了会怎么样？

#### 什么是最左前缀匹配？为什么要遵守？

#### 执行计划中，key 有值，还是很慢怎么办？

#### 数据库乐观锁的过程中，完全没有加任何锁吗？

#### MySQL 的 binlog 有几种格式

#### MySQL 的并行复制原理

#### MySQL 的深度分页如何优化

#### 什么是数据库的主从延迟，如何解决？

#### 什么是事务的 2 阶段提交？

#### 介绍下 MySQL 5.7 中的组提交

#### 为什么 MySQL 8.0 要取消查询缓存？

#### MyISAM 的索引结构是怎么样的，它存在的问题是什么？

#### MySQL 中 like 的模糊查询如何优化

#### 数据库怎么做加密和解密？

#### 数据库加密后怎么做模糊查询？

#### where 条件的顺序影响使用索引吗？

#### 什么是 MySQL 的字典锁？

#### 为什么 MySQL 会选错索引，如何解决？

#### 唯一索引和主键索引的区别？

#### 联合索引是越多越好吗？

#### 为什么不建议使用存储过程?

#### 阿里的数据库能抗秒杀的原理

#### 一个查询语句的执行顺序是怎么样的？

#### on 和 where 有什么区别？

#### InnoDB 中的表级锁、页级锁、行级锁？

#### truncate、delete、drop 的区别？

#### MySQL 的 limit + order by 为什么会数据重复？

#### 什么时候索引失效反而提升效率？

#### 高并发情况下自增主键会不会重复，为什么？

#### 什么情况会导致自增主键不连续？

#### Using filesort 能优化吗，怎么优化？

#### 从 innodb 的索引结构分析，为什么索引的 key 长度不能太长?

#### MySQL 用了函数一定会索引失效吗？

#### 什么是索引合并，原理是什么？

#### a,b 两个单独索引，where a=xx and b=xx 走哪个索引？为什么？

#### 为什么不推荐使用外键？

MySQL 外键（Foreign Key）是用于建立表之间关系的，它定义了一个表中的一列或一组列，这些列的值必须在另一个表的主键列中存在。

MySQL 外键最大的作用就是有助于维护数据的一致性和完整性。

- 一致性：如果一个订单表引用了一个客户表的外键，外键可以确保订单的客户 ID 存在于客户表中，从而保持数据的一致性。
- 完整性：外键可以防止在引用表中删除正在被其他表引用的记录，从而维护数据的完整性。

> 但是，其实在很多大型互联网公司中，很少用外键的，甚至阿里巴巴 Java 开发手册中明确规定了：
>
> 【强制】不得使用外键与级联，一切外键概念必须在应用层解决。

说明: 以学生和成绩的关系为例，学生表中的 student_id 是主键，那么成绩表中的 student_id 则为外键。如果更新学生表中的 student_id，同时触发成绩表中的 student_id 更新，即为级联更新。外键与级联更新适用于单机低并发，不适合分布式、高并发集群;级联更新是强阻塞，存在数据库更新风暴的风险;外键影响数据库的插入速度。

**那么，使用外键会带来哪些问题呢？**

先举个例子，我们有两张表：Orders（订单）和 OrderItems（订单项）。这两个表之间通过外键建立关系，订单项表中的外键引用订单表的订单号。

**性能问题**

首先就是性能问题，因为外键会增加数据库的维护负担，因为每次插入、更新或删除数据时，数据库都需要检查外键约束的完整性。

首先，这两张表中共有两个索引，一个是Orders表的主键索引，一个是 OrdersItems 表的外键索引，这就使得每次插入、更新或删除订单或订单项时，数据库需都要维护这两个索引，这可能会导致性能开销。

其次，在插入新的订单项之前，数据库需要执行数据一致性检查以确保引用的订单号在 Orders 表中存在。这额外的检查可能增加插入订单项的执行时间。

**锁竞争问题**

还有就是比较容易忽略的锁竞争问题。当多个事务同时尝试插入或更新订单项时，它们就需要去检查订单表，就需要获得额外的锁，以确保一致性。这可能导致事务之间的锁竞争，降低并发性能。

一旦有了锁竞争，就可能带来更加严重的死锁问题，所以都是需要尽量避免的。

**无法适应分库分表**

当数据量大的时候，我们就要考虑分库分表了，但是在分库分表环境中，相关数据可能分布在不同的数据库中，外键通常难以跨越不同数据库来建立关系。更重要的是，分库分表环境中，数据的一致性可能更难维护。跨库事务搞不定。


以上，就是一些比较重要的原因吧。其实最主要的还是外键约束会带来一些额外的开销及锁竞争。而在很多大型互联网公司中，都是会尽量避免的。

就像大厂会使用 RC 来替代 RR 一样，会尽可能的降低锁的发生，一方面提升性能，一方面避免死锁。

**逻辑删除**

我们在很多时候，都不会直接删除数据，而是通过逻辑删除进行，而外键在逻辑删除中会有一些约束。如：

1、外键是一种物理约束，而我们业务当中更多用的是逻辑删除。
2、加了外键之后必须先删子表，再删主表，而更多情况下都是先对主表做逻辑删除，再对子表做统一处理。

